{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from datetime import datetime\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from sys import path\n",
    "path.append( join( join( getcwd() , 'functions/' ) ) )\n",
    "\n",
    "from functions import preprocessing, modelling, postprocessing\n",
    "from config import ConfigDict\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# temp\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Run only for the first time#\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data_dir, data, param_grid, read_type, sep, remove_class_0, \n",
    "                make_all_other_classes_1, running_CNN, running_SVM,\n",
    "                timestamp, output_path_vectorizer, store_tfidf_tokenizer, \n",
    "                store_keras_tokenizer, file_path_glove,  debbug = False ):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 1. Ingest Data\n",
    "    #----------------------------------------------------------------#   \n",
    "\n",
    "    if read_type == 'excel':\n",
    "    \n",
    "        corpus = pd.read_excel(input_data, engine='openpyxl')\n",
    "\n",
    "    elif read_type == 'csv':\n",
    "\n",
    "        corpus = pd.read_csv( input_data, sep = sep)\n",
    "\n",
    "    # Filter all NAs values\n",
    "    corpus.dropna(inplace= True)\n",
    "\n",
    "    # Make Sure labels are integers\n",
    "    corpus['label'] = corpus['label'].astype(int)\n",
    "\n",
    "    # Perform data cleaning\n",
    "    corpus = preprocessing.data_cleaning(corpus = corpus,\n",
    "                        sent_tokenizer = False, \n",
    "                        text_cleaning = True, \n",
    "                        use_nltk_cleaning = False)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 2. Preprocess Data\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # Create Filter on the data to avoid the imbalance classes problem\n",
    "    if make_all_other_classes_1:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus['label'] = np.where( corpus['label'] > 0 , 1, corpus['label'])\n",
    "\n",
    "    if remove_class_0:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus = corpus[~corpus['label'].isin([0])]\n",
    "\n",
    "        # Reindex classes\n",
    "        corpus['label'] = corpus['label'].map({1:0,2:1,3:2,4:3})\n",
    "\n",
    "    print(\" The unique labels are \", corpus['label'].unique())\n",
    "\n",
    "    model_data = preprocessing.prepare_training_data(corpus)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    data = {**data, **model_data}\n",
    "\n",
    "    if debbug:\n",
    "        data['corpus'] = corpus\n",
    "        return data, param_grid\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 3. Vectorization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    if running_CNN:\n",
    "    \n",
    "        data['X_train_CNN'], data['X_test_CNN'], data['vocab_size'], data['vocab'] = modelling.keras_tokenizer(num_words = data['num_words'], \n",
    "                                                                                            sentences_train = data['sentences_train_CNN'] , \n",
    "                                                                                            sentences_test = data['sentences_test_CNN'],\n",
    "                                                                                            seq_input_len = data['seq_input_len'],\n",
    "                                                                                            store_keras_tokenizer= store_keras_tokenizer, \n",
    "                                                                                            remove_class_0 = remove_class_0, \n",
    "                                                                                            make_all_other_classes_1 =make_all_other_classes_1, \n",
    "                                                                                            output_path_vectorizer = output_path_vectorizer, \n",
    "                                                                                            timestamp = timestamp)\n",
    "        if use_tfidf_as_embedding_weights:\n",
    "        \n",
    "            data['embedding_matrix'], data['embedding_dim']  = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                        corpus = corpus, \n",
    "                                                                        sentences_train = data['sentences_train_CNN'])\n",
    "        \n",
    "        elif use_glove_pretrained_embeddings_weights:\n",
    "            \n",
    "            data['embedding_matrix'], data['embedding_dim'] = modelling.fit_pretrained_embedding_space_glove(embedding_dim = data['embedding_dim'], \n",
    "                                                                                filepath = data['filepath'] , \n",
    "                                                                                vocab = data['vocab'])\n",
    "\n",
    "    if running_SVM: \n",
    "\n",
    "        data['X_train_SVM'], data['X_test_SVM'], data['vocab_size'], data['vocab'] = modelling.tfidf_tokenizer(num_words = data['num_words'],\n",
    "                                                                                            corpus = corpus,\n",
    "                                                                                            sentences_train = data['sentences_train_SVM'],\n",
    "                                                                                            sentences_test = data['sentences_test_SVM'], \n",
    "                                                                                            timestamp = timestamp, \n",
    "                                                                                            output_path_vectorizer = output_path_vectorizer,\n",
    "                                                                                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                                                                                            remove_class_0 = remove_class_0, \n",
    "                                                                                            make_all_other_classes_1 = make_all_other_classes_1\n",
    "                                                                                            )\n",
    "\n",
    "    if use_tfidf_as_embedding_weights: \n",
    "        \n",
    "        data['embedding_matrix'], data['embedding_dim'] = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                                                    corpus = data['corpus'], \n",
    "                                                                                                    sentences_train_CNN = data['X_train_CNN'])\n",
    "\n",
    "    \n",
    "    if use_glove_pretrained_embeddings_weights:  \n",
    "        \n",
    "        data['embedding_matrix'] = modelling.create_embedding_matrix(\n",
    "                             filepath = file_path_glove,\n",
    "                             word_index = data['vocab'], \n",
    "                             embedding_dim = data['embedding_dim'])\n",
    "\n",
    "    # data_pre = modelling.data_vectorization(sentences_train_CNN = data['sentences_train_CNN'], \n",
    "    #                     sentences_test_CNN = data['sentences_test_CNN'], \n",
    "    #                     sentences_train_SVM = data['sentences_train_SVM'], \n",
    "    #                     sentences_test_SVM = data['sentences_test_SVM'], \n",
    "    #                     num_words = data['num_words'], \n",
    "    #                     seq_input_len = data['seq_input_len'], \n",
    "    #                     filepath = data['filepath'],\n",
    "    #                     corpus = corpus,\n",
    "    #                     vocab = data['vocab'],\n",
    "    #                     embedding_dim = data['embedding_dim'],\n",
    "    #                     running_CNN = running_CNN, \n",
    "    #                     running_SVM = running_SVM, \n",
    "    #                     use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "    #                     use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights, \n",
    "    #                     timestamp = timestamp, \n",
    "    #                     output_path_vectorizer = output_path_vectorizer, \n",
    "    #                     store_tfidf_tokenizer = store_tfidf_tokenizer)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    #data = {**data, **data_pre}\n",
    "\n",
    "    # Add final parameters\n",
    "    param_grid['embedding_matrix'] = ([data['embedding_matrix']]) \n",
    "    param_grid['output_label'] = [data['output_label']]\n",
    "    param_grid['corpus'] = corpus\n",
    "\n",
    "    return data, param_grid\n",
    "\n",
    "\n",
    "def train_SVM(data,C, kernel, degree, gamma, class_weight,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes,\n",
    "            make_all_other_classes_1,\n",
    "            remove_class_0, \n",
    "            store_SVM_model, \n",
    "            timestamp,\n",
    "            output_path_models, \n",
    "            output_path_parameters,\n",
    "            seed):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "\n",
    "    if imbalanced_classes: \n",
    "        \n",
    "        if make_all_other_classes_1: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "\n",
    "        if remove_class_0:\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "            \n",
    "\n",
    "        if not(make_all_other_classes_1 and remove_class_0) and class_weight :\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)    \n",
    "\n",
    "        else: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                    kernel = kernel,\n",
    "                    degree = degree, \n",
    "                    gamma = gamma,\n",
    "                    )\n",
    "\n",
    "    # Fit SVM Model\n",
    "    SVM.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    if store_SVM_model:\n",
    "\n",
    "        file_name = f'SVM'\n",
    "\n",
    "        if remove_class_0:\n",
    "            file_name = f'SVM_1234'\n",
    "\n",
    "        if make_all_other_classes_1:\n",
    "            file_name = f'SVM_01'\n",
    "\n",
    "        print(\"make_all_other_classes_1: \",  make_all_other_classes_1)\n",
    "        print(\"remove_class_0 :\" , remove_class_0)\n",
    "\n",
    "        postprocessing.store_to_pickle(data = SVM, \n",
    "                        output_path = output_path_models, \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = file_name  )\n",
    "\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(data['X_test_SVM'])\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    data['test_acc'] = np.round( accuracy_score(predictions_SVM, data['Y_test_SVM'])*100 , 4)\n",
    "\n",
    "    print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, data['Y_test_SVM'])*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y = SVM.predict(data['X_train_SVM'])\n",
    "    data['conf_matrix'] = confusion_matrix(data['Y_test_SVM'], predictions_SVM)/len(predictions_SVM)\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    data['label_acc'] = postprocessing.cal_label_accuracy(data['conf_matrix'], verbose  = 1)\n",
    "\n",
    "    postprocessing.write_results_txt_SVM( output_file = output_path_parameters,  \n",
    "                      timestamp = timestamp, \n",
    "                      test_acc = data['test_acc'] , \n",
    "                      label_acc = data['label_acc'], \n",
    "                      sent_tokenizer = sent_tokenizer, \n",
    "                      use_nltk_cleaning = use_nltk_cleaning, \n",
    "                      text_cleaning = text_cleaning , \n",
    "                      use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                      use_keras_tokenizer = use_keras_tokenizer, \n",
    "                      use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                      use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                      use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                      imbalanced_classes = imbalanced_classes,\n",
    "                      make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                      remove_class_0 = remove_class_0,\n",
    "                      C = C ,\n",
    "                      kernel = kernel,\n",
    "                      degree = degree, \n",
    "                      gamma = gamma,\n",
    "                      class_weight = class_weight, \n",
    "                      seed = seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_bayes(data, output_path_models, output_path_parameters,  timestamp, store_bayes_model,\n",
    "                make_all_other_classes_1, remove_class_0):\n",
    "\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    if store_bayes_model:\n",
    "\n",
    "        file_name = f'BAYES'\n",
    "\n",
    "        if remove_class_0:\n",
    "            file_name = f'BAYES_1234'\n",
    "\n",
    "        if make_all_other_classes_1:\n",
    "            file_name = f'BAYES_01'\n",
    "\n",
    "        print(\"make_all_other_classes_1: \",  make_all_other_classes_1)\n",
    "        print(\"remove_class_0 :\" , remove_class_0)\n",
    "\n",
    "        postprocessing.store_to_pickle(data = Naive, \n",
    "                        output_path = output_path_models, \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = file_name )\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(data['X_test_SVM'])\n",
    "\n",
    "    data['test_acc'] = np.round( accuracy_score(predictions_NB, data['Y_test_SVM'])*100 , 4)\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, data['Y_test_SVM'])*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y_bayes = Naive.predict(data['X_test_SVM'])\n",
    "    data['conf_matrix'] = confusion_matrix(data['Y_test_SVM'], Pred_Y_bayes)/len(Pred_Y_bayes)\n",
    "\n",
    "\n",
    "    data['label_acc'] = postprocessing.cal_label_accuracy(data['conf_matrix'], verbose  = 1)\n",
    "\n",
    "\n",
    "    postprocessing.write_results_txt_BAYES( output_file = output_path_parameters,  \n",
    "                timestamp = timestamp, \n",
    "                test_acc = data['test_acc'] , \n",
    "                label_acc = data['label_acc'], \n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning , \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                remove_class_0 = remove_class_0,\n",
    "                seed = seed)\n"
   ]
  },
  {
   "source": [
    "## Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigDict.read('config/config_param.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'params': {'model': {'running_CNN': True,\n",
       "   'running_SVM': True,\n",
       "   'running_bayes': True,\n",
       "   'seed': 123},\n",
       "  'input_data': {'data': 'data/ML_data_2.0.xlsx',\n",
       "   'sep': ',',\n",
       "   'read_type': 'excel'},\n",
       "  'output_data': {'output_path_vectorizer': 'results/vectorizer/',\n",
       "   'output_path_model': 'results/model/',\n",
       "   'output_parameters': 'results/parameters/',\n",
       "   'store_tfidf_tokenizer': True,\n",
       "   'store_keras_tokenizer': True,\n",
       "   'store_SVM_model': True,\n",
       "   'store_CNN_model': True,\n",
       "   'store_bayes_model': True},\n",
       "  'tokenization_options': {'sent_tokenizer': False,\n",
       "   'use_nltk_cleaning': True,\n",
       "   'text_cleaning': False,\n",
       "   'use_tfidf_tokenizer': True,\n",
       "   'use_keras_tokenizer': True,\n",
       "   'use_pretrained_embeddings': True,\n",
       "   'use_glove_pretrained_embeddings_weights': True,\n",
       "   'use_tfidf_as_embedding_weights': False,\n",
       "   'imbalanced_classes': True,\n",
       "   'make_all_other_classes_1': True,\n",
       "   'remove_class_0': True},\n",
       "  'CNN': {'epochs': 30,\n",
       "   'batch_size': 16,\n",
       "   'num_words': 5000,\n",
       "   'cv': 4,\n",
       "   'n_iter': 5,\n",
       "   'seq_input_len': 40,\n",
       "   'embedding_dim': 40,\n",
       "   'nodes_hidden_dense_layer': 5,\n",
       "   'filepath_GloVe': 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt',\n",
       "   'grid_search': {'num_filters_cv': [[64, 16], [64, 32], [128, 16], [128, 32], [256, 64], [256, 32], [256, 64], [512, 128], [512, 32]],\n",
       "    'kernel_size_cv': [[2, 3], [2, 4], [3, 4], [3, 5]],\n",
       "    'vocab_size': [3000, 4000, 5000, 6000],\n",
       "    'embedding_dim': [20, 30, 40, 50],\n",
       "    'seq_input_len': [50, 40, 30, 20, 10],\n",
       "    'nodes_hidden_dense_layer': [5, 10, 15, 20, 40],\n",
       "    'use_pretrained_embeddings': [True, False]}},\n",
       "  'SVM': {'C': 1.0,\n",
       "   'kernel': 'linear',\n",
       "   'degree': 3,\n",
       "   'gamma': 'auto',\n",
       "   'use_class_weights': True,\n",
       "   'class_weights': {'0': 0.05, '1': 1, '2': 1, '3': 1, '4': 1},\n",
       "   'class_weights_2': {'0': 0.25, '1': 1},\n",
       "   'class_weights_1_2_3_4': {'0': 1, '1': 1, '2': 1, '3': 1}}}}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "source": [
    "## Run Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------#\n",
    "# 0. Parameters\n",
    "# ----------------------------------------------------------------#\n",
    "\n",
    "# Define Timestamp to store models and vectorizer\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# output paths \n",
    "output_path_vectorizer = config['params']['output_data']['output_path_vectorizer']\n",
    "output_path_models = config['params']['output_data']['output_path_model']\n",
    "output_path_parameters = config['params']['output_data']['output_parameters']\n",
    "\n",
    "# Storage options\n",
    "store_tfidf_tokenizer = config['params']['output_data']['store_tfidf_tokenizer']\n",
    "store_keras_tokenizer = config['params']['output_data']['store_keras_tokenizer']\n",
    "store_SVM_model = config['params']['output_data']['store_SVM_model']\n",
    "store_bayes_model = config['params']['output_data']['store_bayes_model']\n",
    "\n",
    "# Set seed\n",
    "seed = config['params']['model']['seed']\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Current Date\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "\n",
    "# Input data\n",
    "input_data = config['params']['input_data']['data']\n",
    "read_type  = config['params']['input_data']['read_type']\n",
    "\n",
    "# Models to run\n",
    "running_CNN = config['params']['model']['running_CNN']\n",
    "running_SVM = config['params']['model']['running_SVM']\n",
    "running_bayes = config['params']['model']['running_bayes']\n",
    "\n",
    "# To test for only clases 0 and 1\n",
    "make_all_other_classes_1 =  config['params']['tokenization_options']['make_all_other_classes_1']\n",
    "remove_class_0 = config['params']['tokenization_options']['remove_class_0']\n",
    "\n",
    "\n",
    "# Sentence Tokenizer\n",
    "sent_tokenizer = config['params']['tokenization_options']['sent_tokenizer'] # TODO: Adjust for input to CNN\n",
    "\n",
    "# Text Cleaning Options\n",
    "use_nltk_cleaning = config['params']['tokenization_options']['use_nltk_cleaning']\n",
    "text_cleaning = config['params']['tokenization_options']['text_cleaning']\n",
    "\n",
    "# Word Tokenizer Options\n",
    "use_tfidf_tokenizer = config['params']['tokenization_options']['use_tfidf_tokenizer'] # For SVM\n",
    "use_keras_tokenizer = config['params']['tokenization_options']['use_keras_tokenizer'] # For CNN\n",
    "\n",
    "# If set to FALSE then keras embedding space training is used instead\n",
    "# Embedding Space possibilites are GloVe or TFIDF\n",
    "use_pretrained_embeddings = config['params']['tokenization_options']['use_pretrained_embeddings']\n",
    "\n",
    "# Only if use_pretrained_embeddings == True then select embedding vector space type\n",
    "use_glove_pretrained_embeddings_weights = config['params']['tokenization_options']['use_glove_pretrained_embeddings_weights']\n",
    "use_tfidf_as_embedding_weights = config['params']['tokenization_options']['use_tfidf_as_embedding_weights']\n",
    "\n",
    "# Options for SVM\n",
    "imbalanced_classes = config['params']['tokenization_options']['imbalanced_classes']\n",
    "C = config['params']['SVM']['C']\n",
    "kernel = config['params']['SVM']['kernel']\n",
    "degree = config['params']['SVM']['degree']\n",
    "gamma = config['params']['SVM']['gamma']\n",
    "\n",
    "# Define Class Weights as empty dict\n",
    "class_weight = {}\n",
    "\n",
    "\n",
    "# Dictionary which will cotain all the model's variables\n",
    "data = {}\n",
    "\n",
    "# Initialize Model\n",
    "data['epochs'] = config['params']['CNN']['epochs'] # NO. of optimization runs\n",
    "data['batch_size'] = config['params']['CNN']['batch_size'] # No. of sentences batch to train\n",
    "data['num_words'] = config['params']['CNN']['num_words'] # No. of words to use in the embedding space of GloVe or TFIDF\n",
    "data['cv'] = config['params']['CNN']['cv'] # No. of Cross Validations\n",
    "data['n_iter'] = config['params']['CNN']['n_iter'] # No. of Iterations\n",
    "data['seq_input_len'] = config['params']['CNN']['seq_input_len'] # Length of the vector sentence ( no. of words per sentence)\n",
    "data['embedding_dim'] = config['params']['CNN']['embedding_dim'] # Length of the word vector ( dimension in the embedding space)\n",
    "data['nodes_hidden_dense_layer'] = config['params']['CNN']['nodes_hidden_dense_layer'] # No. of nodes for hidden Dense layer\n",
    "\n",
    "\n",
    "data['filepath'] = config['params']['CNN']['filepath_GloVe'] # File path to GLoVe pretrained embedding words\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "param_grid = dict(num_filters_cv = config['params']['CNN']['grid_search']['num_filters_cv'], # No of filter to use in convolution\n",
    "                kernel_size_cv = config['params']['CNN']['grid_search']['kernel_size_cv'], # No of words to check per Convolution \n",
    "                vocab_size = config['params']['CNN']['grid_search']['vocab_size'], # Vocab size if keras embedding space training is wanted\n",
    "                embedding_dim = config['params']['CNN']['grid_search']['embedding_dim'], \n",
    "                seq_input_len = config['params']['CNN']['grid_search']['seq_input_len'], \n",
    "                nodes_hidden_dense_layer = config['params']['CNN']['grid_search']['nodes_hidden_dense_layer'],\n",
    "                use_pretrained_embeddings = config['params']['CNN']['grid_search']['use_pretrained_embeddings']\n",
    "                )\n",
    "# Small Test\n",
    "param_grid = dict(num_filters_cv = [(64,16)],\n",
    "                    kernel_size_cv = [(2,3)],\n",
    "                    vocab_size = [5000], \n",
    "                    embedding_dim = [50],\n",
    "                    seq_input_len = [50], \n",
    "                    nodes_hidden_dense_layer = [5],\n",
    "                    use_pretrained_embeddings = [config['params']['CNN']['grid_search']['use_pretrained_embeddings']])\n"
   ]
  },
  {
   "source": [
    "## Case 1: Model for Labels 0 and make 1, 2, 3, 4 equal to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 1]\n",
      "make_all_other_classes_1:  True\n",
      "remove_class_0 : False\n",
      "-------------------------------------------\n",
      "Naive Bayes Accuracy Score ->  83.06772908366534\n",
      "Accuracy for label 0 :  98.64  %\n",
      "Accuracy for label 1 :  40.3  %\n",
      "Writting results...\n",
      "Running BAYES Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 83.0677\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: True\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_pretrained_embeddings: True\n",
      " \n",
      "            label accuracy: {0: 98.64, 1: 40.3}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "if make_all_other_classes_1:\n",
    "\n",
    "    remove_class_0_case_1 = False\n",
    "\n",
    "    class_weight = {0: config['params']['SVM']['class_weights_2']['0'],\n",
    "                    1: config['params']['SVM']['class_weights_2']['1']}\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_1,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            store_keras_tokenizer = store_keras_tokenizer,\n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1,\n",
    "            remove_class_0 = remove_class_0_case_1, \n",
    "            seed = seed, \n",
    "            store_SVM_model = store_SVM_model,\n",
    "            timestamp = timestamp,\n",
    "            output_path_models = output_path_models, \n",
    "            output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    if running_bayes:\n",
    "\n",
    "        train_bayes(data = data, \n",
    "                    output_path_models = output_path_models, \n",
    "                    timestamp = timestamp,\n",
    "                    store_bayes_model = store_bayes_model ,\n",
    "                    output_path_parameters = output_path_parameters, \n",
    "                    make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                    remove_class_0 = remove_class_0_case_1)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Case 2: Model for 1, 2, 3, 4  and exclude 0. Convert 1, 2, 3 , 4 to 0, 1, 2, 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [3 0 2 1]\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : True\n",
      "SVM Accuracy Score ->  71.85185185185186\n",
      "Accuracy for label 0 :  62.86  %\n",
      "Accuracy for label 1 :  63.89  %\n",
      "Accuracy for label 2 :  77.14  %\n",
      "Accuracy for label 3 :  86.21  %\n",
      "Writting results...\n",
      "Running SVM Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 71.8519\n",
      "\n",
      "            C : 1.0\n",
      "\n",
      "            kernel : linear\n",
      "\n",
      "            degree : 3\n",
      " \n",
      "            gamma : auto\n",
      "\n",
      "            class_weight : {0: 1, 1: 1, 2: 1, 3: 1}\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: True \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_keras_tokenizer: True\n",
      " \n",
      "            use_pretrained_embeddings: True\n",
      " \n",
      "            use_glove_pretrained_embeddings_weights: True\n",
      " \n",
      "            use_tfidf_as_embedding_weights: False\n",
      " \n",
      "            imbalanced_classes: True\n",
      " \n",
      "            label accuracy: {0: 62.86, 1: 63.89, 2: 77.14, 3: 86.21}\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : True\n",
      "-------------------------------------------\n",
      "Naive Bayes Accuracy Score ->  61.48148148148148\n",
      "Accuracy for label 0 :  34.29  %\n",
      "Accuracy for label 1 :  61.11  %\n",
      "Accuracy for label 2 :  88.57  %\n",
      "Accuracy for label 3 :  62.07  %\n",
      "Writting results...\n",
      "Running BAYES Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 61.4815\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: True \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_pretrained_embeddings: True\n",
      " \n",
      "            label accuracy: {0: 34.29, 1: 61.11, 2: 88.57, 3: 62.07}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if remove_class_0:\n",
    "\n",
    "    make_all_other_classes_1_case_2 = False\n",
    "    \n",
    "    # Remember that without the 0 , the other labels are reindexed\n",
    "    class_weight = {0: config['params']['SVM']['class_weights_1_2_3_4']['0'],\n",
    "                    1: config['params']['SVM']['class_weights_1_2_3_4']['1'],\n",
    "                    2: config['params']['SVM']['class_weights_1_2_3_4']['2'],\n",
    "                    3: config['params']['SVM']['class_weights_1_2_3_4']['3']}\n",
    "\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_2, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            store_keras_tokenizer = store_keras_tokenizer,\n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1_case_2,\n",
    "            remove_class_0 = remove_class_0, \n",
    "            seed = seed, \n",
    "            store_SVM_model = store_SVM_model,\n",
    "            timestamp = timestamp,\n",
    "            output_path_models = output_path_models, \n",
    "            output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    if running_bayes:\n",
    "\n",
    "        train_bayes(data = data, \n",
    "                    output_path_models = output_path_models, \n",
    "                    timestamp = timestamp,\n",
    "                    store_bayes_model = store_bayes_model ,\n",
    "                    output_path_parameters = output_path_parameters, \n",
    "                    make_all_other_classes_1 = make_all_other_classes_1_case_2, \n",
    "                    remove_class_0 = remove_class_0)\n"
   ]
  },
  {
   "source": [
    "## Case 3: Model for all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 4 1 3 2]\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : False\n",
      "SVM Accuracy Score ->  70.91633466135458\n",
      "Accuracy for label 0 :  73.04  %\n",
      "Accuracy for label 1 :  41.67  %\n",
      "Accuracy for label 2 :  60.98  %\n",
      "Accuracy for label 3 :  86.36  %\n",
      "Accuracy for label 4 :  64.58  %\n",
      "Writting results...\n",
      "Running SVM Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 70.9163\n",
      "\n",
      "            C : 1.0\n",
      "\n",
      "            kernel : linear\n",
      "\n",
      "            degree : 3\n",
      " \n",
      "            gamma : auto\n",
      "\n",
      "            class_weight : {0: 0.05, 1: 1, 2: 1, 3: 1, 4: 1}\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_keras_tokenizer: True\n",
      " \n",
      "            use_pretrained_embeddings: True\n",
      " \n",
      "            use_glove_pretrained_embeddings_weights: True\n",
      " \n",
      "            use_tfidf_as_embedding_weights: False\n",
      " \n",
      "            imbalanced_classes: True\n",
      " \n",
      "            label accuracy: {0: 73.04, 1: 41.67, 2: 60.98, 3: 86.36, 4: 64.58}\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : False\n",
      "-------------------------------------------\n",
      "Naive Bayes Accuracy Score ->  68.72509960159363\n",
      "Accuracy for label 0 :  100.0  %\n",
      "Accuracy for label 1 :  0.0  %\n",
      "Accuracy for label 2 :  0.0  %\n",
      "Accuracy for label 3 :  0.0  %\n",
      "Accuracy for label 4 :  0.0  %\n",
      "Writting results...\n",
      "Running BAYES Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 68.7251\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_pretrained_embeddings: True\n",
      " \n",
      "            label accuracy: {0: 100.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    remove_class_0_case_3 = False\n",
    "    make_all_other_classes_1_case_3 = False\n",
    "\n",
    "    if config['params']['SVM']['use_class_weights']:\n",
    "\n",
    "        class_weight = {0: config['params']['SVM']['class_weights']['0'],\n",
    "                    1: config['params']['SVM']['class_weights']['1'],\n",
    "                    2: config['params']['SVM']['class_weights']['2'],\n",
    "                    3: config['params']['SVM']['class_weights']['3'],\n",
    "                    4: config['params']['SVM']['class_weights']['4']}\n",
    "\n",
    "\n",
    "\n",
    "        data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_3,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_3, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            store_keras_tokenizer = store_keras_tokenizer,\n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "            # Train and Calculate Accuracy for SVM\n",
    "        if running_SVM:\n",
    "\n",
    "            train_SVM(data = data, \n",
    "                    C = C, \n",
    "                    kernel = kernel, \n",
    "                    degree = degree, \n",
    "                    gamma = gamma, \n",
    "                    class_weight = class_weight,\n",
    "                    sent_tokenizer = sent_tokenizer, \n",
    "                    use_nltk_cleaning = use_nltk_cleaning, \n",
    "                    text_cleaning = text_cleaning, \n",
    "                    use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                    use_keras_tokenizer = use_keras_tokenizer, \n",
    "                    use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                    use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                    use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                    imbalanced_classes = imbalanced_classes,\n",
    "                    make_all_other_classes_1 = make_all_other_classes_1_case_3,\n",
    "                    remove_class_0 = remove_class_0_case_3, \n",
    "                    seed = seed, \n",
    "                    store_SVM_model = store_SVM_model,\n",
    "                    timestamp = timestamp,\n",
    "                    output_path_models = output_path_models, \n",
    "                    output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "        if running_bayes:\n",
    "\n",
    "           train_bayes(data = data, \n",
    "                    output_path_models = output_path_models, \n",
    "                    timestamp = timestamp,\n",
    "                    store_bayes_model = store_bayes_model ,\n",
    "                    output_path_parameters = output_path_parameters, \n",
    "                    make_all_other_classes_1 = make_all_other_classes_1_case_3, \n",
    "                    remove_class_0 = remove_class_0_case_3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  raw materials purchase is decisive for the com...      1\n",
       "1      framework agreement master contract preferred      0\n",
       "2  twostep process                               ...      2\n",
       "3  companys top management promptly ordered chang...      3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>raw materials purchase is decisive for the com...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>framework agreement master contract preferred</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>twostep process                               ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>companys top management promptly ordered chang...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "new_sentence  = pd.DataFrame({'text':[\"raw materials purchase is decisive for the company\", 'framework agreement master contract preferred', 'twostep process                                        buying leverage established provoking leverage portfolio matrix' , 'companys top management promptly ordered change purchasing policy build alternative domestic sources'],\n",
    "                            'label':[1,0,2,3]})\n",
    "\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-01-17_22-37-56'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Test Case 1\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_01') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 1, 1, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Test Case 2\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer_1234')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_1234') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Test Case 3\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('class': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a859f67d4acffa6abd10af9224a6e751dd4159d06149102e129e99dcb493c1c8"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}