{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from datetime import datetime\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from sys import path\n",
    "path.append( join( join( getcwd() , 'functions/' ) ) )\n",
    "\n",
    "from functions import preprocessing, modelling, postprocessing\n",
    "from config import ConfigDict\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# Run only for the first time#\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data_dir, data, param_grid, read_type, sep, remove_class_0, \n",
    "                make_all_other_classes_1, running_CNN, running_SVM ):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 1. Ingest Data\n",
    "    #----------------------------------------------------------------#   \n",
    "\n",
    "    if read_type == 'excel':\n",
    "    \n",
    "        corpus = pd.read_excel(input_data, engine='openpyxl')\n",
    "\n",
    "    elif read_type == 'csv':\n",
    "\n",
    "        corpus = pd.read_csv( input_data, sep = sep)\n",
    "\n",
    "    # Filter all NAs values\n",
    "    corpus.dropna(inplace= True)\n",
    "\n",
    "    # Make Sure labels are integers\n",
    "    corpus['label'] = corpus['label'].astype(int)\n",
    "\n",
    "    # Perform data cleaning\n",
    "    corpus = preprocessing.data_cleaning(corpus = corpus,\n",
    "                        sent_tokenizer = False, \n",
    "                        text_cleaning = True, \n",
    "                        use_nltk_cleaning = False)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 2. Preprocess Data\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # Create Filter on the data to avoid the imbalance classes problem\n",
    "    if make_all_other_classes_1:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus['label'] = np.where( corpus['label'] > 0 , 1, corpus['label'])\n",
    "\n",
    "    if remove_class_0:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus = corpus[~corpus['label'].isin([0])]\n",
    "\n",
    "        # Reindex classes\n",
    "        corpus['label'] = corpus['label'].map({1:0,2:1,3:2,4:3})\n",
    "\n",
    "    print(\" The unique labels are \", corpus['label'].unique())\n",
    "\n",
    "    model_data = preprocessing.prepare_training_data(corpus)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    data = {**data, **model_data}\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 3. Vectorization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    if running_CNN:\n",
    "    \n",
    "        data['X_train_CNN'], data['X_test_CNN'], data['vocab_size'], data['vocab'] = modelling.keras_tokenizer(num_words = data['num_words'], \n",
    "                                                                                            sentences_train = data['sentences_train_CNN'] , \n",
    "                                                                                            sentences_test = data['sentences_test_CNN'],\n",
    "                                                                                            seq_input_len = data['seq_input_len'])\n",
    "        if use_tfidf_as_embedding_weights:\n",
    "        \n",
    "            data['embedding_matrix'], data['embedding_dim']  = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                        corpus = corpus, \n",
    "                                                                        sentences_train = data['sentences_train_CNN'])\n",
    "        \n",
    "        elif use_glove_pretrained_embeddings_weights:\n",
    "            \n",
    "            data['embedding_matrix'], data['embedding_dim'] = modelling.fit_pretrained_embedding_space_glove(embedding_dim = data['embedding_dim'], \n",
    "                                                                                filepath = data['filepath'] , \n",
    "                                                                                vocab = data['vocab'])\n",
    "\n",
    "    if running_SVM: \n",
    "\n",
    "        data['X_train_SVM'], data['X_test_SVM'], data['vocab_size'], data['vocab'] = modelling.tfidf_tokenizer(num_words = data['num_words'],\n",
    "                                                                                            corpus = corpus,\n",
    "                                                                                            sentences_train = data['sentences_train_SVM'],\n",
    "                                                                                            sentences_test = data['sentences_test_SVM'])\n",
    "    \n",
    "\n",
    "\n",
    "    data_pre = modelling.data_vectorization(sentences_train_CNN = data['sentences_train_CNN'], \n",
    "                        sentences_test_CNN = data['sentences_test_CNN'], \n",
    "                        sentences_train_SVM = data['sentences_train_SVM'], \n",
    "                        sentences_test_SVM = data['sentences_test_SVM'], \n",
    "                        num_words = data['num_words'], \n",
    "                        seq_input_len = data['seq_input_len'], \n",
    "                        filepath = data['filepath'],\n",
    "                        corpus = corpus,\n",
    "                        vocab = data['vocab'],\n",
    "                        embedding_dim = data['embedding_dim'],\n",
    "                        running_CNN = running_CNN, \n",
    "                        running_SVM = running_SVM, \n",
    "                        use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                        use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    data = {**data, **data_pre}\n",
    "\n",
    "    # Add final parameters\n",
    "    param_grid['embedding_matrix'] = ([data['embedding_matrix']])\n",
    "    param_grid['output_label'] = [data['output_label']]\n",
    "    param_grid['corpus'] = corpus\n",
    "\n",
    "    return data, param_grid\n",
    "\n",
    "\n",
    "def train_SVM(data,C, kernel, degree, gamma, class_weight,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes,\n",
    "            make_all_other_classes_1,\n",
    "            remove_class_0):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "\n",
    "    if imbalanced_classes: \n",
    "        \n",
    "        if make_all_other_classes_1: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "\n",
    "        if remove_class_0:\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "            \n",
    "\n",
    "        if not(make_all_other_classes_1 and remove_class_0) and class_weight :\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)    \n",
    "\n",
    "        else: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                    kernel = kernel,\n",
    "                    degree = degree, \n",
    "                    gamma = gamma,\n",
    "                    )\n",
    "\n",
    "    # Fit SVM Model\n",
    "    SVM.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(data['X_test_SVM'])\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    data['test_acc'] = np.round( accuracy_score(predictions_SVM, data['Y_test_SVM'])*100 , 4)\n",
    "\n",
    "    print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, data['Y_test_SVM'])*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y = SVM.predict(data['X_train_SVM'])\n",
    "    data['conf_matrix'] = confusion_matrix(data['Y_test_SVM'], predictions_SVM)/len(predictions_SVM)\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    data['label_acc'] = postprocessing.cal_label_accuracy(data['conf_matrix'], verbose  = 1)\n",
    "\n",
    "    postprocessing.write_results_txt_SVM( output_file = data['output_file'],  \n",
    "                      test_acc = data['test_acc'] , \n",
    "                      label_acc = data['label_acc'], \n",
    "                      sent_tokenizer = sent_tokenizer, \n",
    "                      use_nltk_cleaning = use_nltk_cleaning, \n",
    "                      text_cleaning = text_cleaning , \n",
    "                      use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                      use_keras_tokenizer = use_keras_tokenizer, \n",
    "                      use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                      use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                      use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                      imbalanced_classes = imbalanced_classes,\n",
    "                      make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                      remove_class_0 = remove_class_0,\n",
    "                      C = C ,\n",
    "                      kernel = kernel,\n",
    "                      degree = degree, \n",
    "                      gamma = gamma,\n",
    "                      class_weight = class_weight )\n",
    "\n",
    "\n",
    "\n",
    "def train_CNN(data, param_grid,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # Run CNN with Hyperparameter Optimization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    model_output = modelling.hyperparameter_optimization( \n",
    "                                    X_train = data['X_train_CNN'], \n",
    "                                    Y_train = data['Y_train_CNN'], \n",
    "                                    X_test = data['X_test_CNN'], \n",
    "                                    Y_test = data['Y_test_CNN'] , \n",
    "                                    epochs = data['epochs'] , \n",
    "                                    batch_size = data['batch_size'],\n",
    "                                    param_grid = param_grid,\n",
    "                                    cv = data['cv'], \n",
    "                                    n_iter = data['n_iter'],\n",
    "                                    verbose = False)\n",
    "\n",
    "    # 5. Score Analysis\n",
    "\n",
    "    # Generate Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(model_output['Y_pred'], data['Y_test_CNN'].argmax(axis=1)) / len(model_output['Y_pred'])\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    model_output['label_acc'] = postprocessing.cal_label_accuracy(conf_matrix, verbose  = 1)\n",
    "\n",
    "    # 6. Write Results to text file\n",
    "    postprocessing.write_results_txt_CNN(output_file = data['output_file'], \n",
    "                best_train_acc = model_output['best_train_acc'], \n",
    "                best_train_param = model_output['best_train_param'],\n",
    "                test_acc = model_output['test_acc'], \n",
    "                label_acc = model_output['label_acc'] , \n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning , \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_keras_tokenizer = use_keras_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                epochs = data['epochs'],\n",
    "                batch_size = data['batch_size'],\n",
    "                num_words = data['num_words'], \n",
    "                cv = data['cv'] ,\n",
    "                n_iter = data['n_iter']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigDict.read('config/config_param.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'params': {'model': {'running_CNN': True, 'running_SVM': True, 'seed': 123},\n",
       "  'input_data': {'data': 'data/ML_data_2.0.xlsx',\n",
       "   'sep': ',',\n",
       "   'read_type': 'excel'},\n",
       "  'tokenization_options': {'sent_tokenizer': False,\n",
       "   'use_nltk_cleaning': True,\n",
       "   'text_cleaning': False,\n",
       "   'use_tfidf_tokenizer': True,\n",
       "   'use_keras_tokenizer': False,\n",
       "   'use_pretrained_embeddings': False,\n",
       "   'use_glove_pretrained_embeddings_weights': False,\n",
       "   'use_tfidf_as_embedding_weights': True,\n",
       "   'imbalanced_classes': True,\n",
       "   'make_all_other_classes_1': True,\n",
       "   'remove_class_0': True},\n",
       "  'data': {'epochs': 30,\n",
       "   'batch_size': 10,\n",
       "   'num_words': 5000,\n",
       "   'cv': 4,\n",
       "   'n_iter': 5,\n",
       "   'seq_input_len': 40,\n",
       "   'embedding_dim': 40,\n",
       "   'nodes_hidden_dense_layer': 5,\n",
       "   'filepath': 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt',\n",
       "   'SVM': {'C': 1.0,\n",
       "    'kernel': 'linear',\n",
       "    'degree': 3,\n",
       "    'gamma': 'auto',\n",
       "    'use_class_weights': True,\n",
       "    'class_weights': {'0': 0.05, '1': 1, '2': 1, '3': 1, '4': 1},\n",
       "    'class_weights_2': {'0': 0.25, '1': 1},\n",
       "    'class_weights_1_2_3_4': {'0': 1, '1': 1, '2': 1, '3': 1}}},\n",
       "  'hyperparam': {'num_filters_cv': [[64, 16], [64, 32], [128, 16], [128, 32], [256, 64], [256, 32], [256, 64], [512, 128], [512, 32]],\n",
       "   'kernel_size_cv': [[2, 3], [2, 4], [3, 4], [3, 5]],\n",
       "   'vocab_size': [3000, 4000, 5000, 6000],\n",
       "   'embedding_dim': [20, 30, 40, 50],\n",
       "   'seq_input_len': [50, 40, 30, 20, 10],\n",
       "   'nodes_hidden_dense_layer': [5, 10, 15, 20, 40],\n",
       "   'use_pretrained_embeddings': [True, False]}}}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "source": [
    "## Run Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------#\n",
    "# 0. Parameters\n",
    "# ----------------------------------------------------------------#\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(config['params']['model']['seed'])\n",
    "\n",
    "# Current Date\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "\n",
    "# Input data\n",
    "input_data = config['params']['input_data']['data']\n",
    "read_type  = config['params']['input_data']['read_type']\n",
    "\n",
    "# Models to run\n",
    "running_CNN = config['params']['model']['running_CNN']\n",
    "running_SVM = config['params']['model']['running_SVM']\n",
    "\n",
    "# To test for only clases 0 and 1\n",
    "make_all_other_classes_1 =  config['params']['tokenization_options']['make_all_other_classes_1']\n",
    "remove_class_0 = config['params']['tokenization_options']['remove_class_0']\n",
    "\n",
    "\n",
    "# Sentence Tokenizer\n",
    "sent_tokenizer = config['params']['tokenization_options']['sent_tokenizer'] # TODO: Adjust for input to CNN\n",
    "\n",
    "# Text Cleaning Options\n",
    "use_nltk_cleaning = config['params']['tokenization_options']['use_nltk_cleaning']\n",
    "text_cleaning = config['params']['tokenization_options']['text_cleaning']\n",
    "\n",
    "# Word Tokenizer Options\n",
    "use_tfidf_tokenizer = config['params']['tokenization_options']['use_tfidf_tokenizer'] # For SVM\n",
    "use_keras_tokenizer = config['params']['tokenization_options']['use_keras_tokenizer'] # For CNN\n",
    "\n",
    "# If set to FALSE then keras embedding space training is used instead\n",
    "# Embedding Space possibilites are GloVe or TFIDF\n",
    "use_pretrained_embeddings = config['params']['tokenization_options']['use_pretrained_embeddings']\n",
    "\n",
    "# Only if use_pretrained_embeddings == True then select embedding vector space type\n",
    "use_glove_pretrained_embeddings_weights = config['params']['tokenization_options']['use_glove_pretrained_embeddings_weights']\n",
    "use_tfidf_as_embedding_weights = config['params']['tokenization_options']['use_tfidf_as_embedding_weights']\n",
    "\n",
    "# Options for SVM\n",
    "imbalanced_classes = config['params']['tokenization_options']['imbalanced_classes']\n",
    "C = config['params']['data']['SVM']['C']\n",
    "kernel = config['params']['data']['SVM']['kernel']\n",
    "degree = config['params']['data']['SVM']['degree']\n",
    "gamma = config['params']['data']['SVM']['gamma']\n",
    "\n",
    "# Define Class Weights as empty dict\n",
    "class_weight = {}\n",
    "\n",
    "\n",
    "# Dictionary which will cotain all the model's variables\n",
    "data = {}\n",
    "\n",
    "# Initialize Model\n",
    "data['epochs'] = config['params']['data']['epochs'] # NO. of optimization runs\n",
    "data['batch_size'] = config['params']['data']['batch_size'] # No. of sentences batch to train\n",
    "data['num_words'] = config['params']['data']['num_words'] # No. of words to use in the embedding space of GloVe or TFIDF\n",
    "data['cv'] = config['params']['data']['cv'] # No. of Cross Validations\n",
    "data['n_iter'] = config['params']['data']['n_iter'] # No. of Iterations\n",
    "data['seq_input_len'] = config['params']['data']['seq_input_len'] # Length of the vector sentence ( no. of words per sentence)\n",
    "data['embedding_dim'] = config['params']['data']['embedding_dim'] # Length of the word vector ( dimension in the embedding space)\n",
    "data['nodes_hidden_dense_layer'] = config['params']['data']['nodes_hidden_dense_layer'] # No. of nodes for hidden Dense layer\n",
    "\n",
    "\n",
    "data['filepath'] = config['params']['data']['filepath'] # File path to GLoVe pretrained embedding words\n",
    "data['output_file'] = f\"results/{current_time}_Result\" # Name of output result file\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "param_grid = dict(num_filters_cv = config['params']['hyperparam']['num_filters_cv'], # No of filter to use in convolution\n",
    "                kernel_size_cv = config['params']['hyperparam']['kernel_size_cv'], # No of words to check per Convolution \n",
    "                vocab_size = config['params']['hyperparam']['vocab_size'], # Vocab size if keras embedding space training is wanted\n",
    "                embedding_dim = config['params']['hyperparam']['embedding_dim'], \n",
    "                seq_input_len = config['params']['hyperparam']['seq_input_len'], \n",
    "                nodes_hidden_dense_layer = config['params']['hyperparam']['nodes_hidden_dense_layer'],\n",
    "                use_pretrained_embeddings = config['params']['hyperparam']['use_pretrained_embeddings']\n",
    "                )\n",
    "#Small Test\n",
    "# param_grid = dict(num_filters_cv = [(64,16)],\n",
    "#                     kernel_size_cv = [(2,3)],\n",
    "#                     vocab_size = [5000], \n",
    "#                     embedding_dim = [50],\n",
    "#                     seq_input_len = [50], \n",
    "#                     nodes_hidden_dense_layer = [5],\n",
    "#                     use_pretrained_embeddings = [True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 1]\n",
      "SVM Accuracy Score ->  86.85258964143426\n",
      "Accuracy for label 0 :  87.09  %\n",
      "Accuracy for label 1 :  86.23  %\n",
      "Writting results...\n",
      "Running SVM Modeling \n",
      "  \n",
      "            Test Accuracy : 86.8526\n",
      "\n",
      "            C : 1.0\n",
      "\n",
      "            kernel : linear\n",
      "\n",
      "            degree : 3\n",
      " \n",
      "            gamma : auto\n",
      "\n",
      "            class_weight : {0: 0.25, 1: 1}\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: True\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_keras_tokenizer: False\n",
      " \n",
      "            use_pretrained_embeddings: False\n",
      " \n",
      "            use_glove_pretrained_embeddings_weights: False\n",
      " \n",
      "            use_tfidf_as_embedding_weights: True\n",
      " \n",
      "            imbalanced_classes: True\n",
      " \n",
      "            label accuracy: {0: 87.09, 1: 86.23}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "\n",
    "if make_all_other_classes_1:\n",
    "\n",
    "\n",
    "    class_weight = {0: config['params']['data']['SVM']['class_weights_2']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights_2']['1']}\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                                data = data, \n",
    "                                param_grid = param_grid, \n",
    "                                read_type = config['params']['input_data']['read_type'], \n",
    "                                sep = config['params']['input_data']['read_type'],\n",
    "                                remove_class_0 = False,\n",
    "                                make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                                running_CNN = running_CNN, \n",
    "                                running_SVM = running_SVM)\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1,\n",
    "            remove_class_0 = remove_class_0)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    # if running_CNN:\n",
    "        \n",
    "    #     train_CNN(data = data, \n",
    "    #         param_grid = param_grid,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [3 0 2 1]\nSVM Accuracy Score ->  75.55555555555556\nAccuracy for label 0 :  56.25  %\nAccuracy for label 1 :  65.63  %\nAccuracy for label 2 :  89.47  %\nAccuracy for label 3 :  87.88  %\nWritting results...\nRunning SVM Modeling \n  \n            Test Accuracy : 75.5556\n\n            C : 1.0\n\n            kernel : linear\n\n            degree : 3\n \n            gamma : auto\n\n            class_weight : {0: 1, 1: 1, 2: 1, 3: 1}\n\n            sent_tokenizer : False \n   \n            use_nltk_cleaning: True\n \n            text_cleaning: False\n  \n            make_all_other_classes_1: True\n  \n            remove_class_0: True \n\n            use_tfidf_tokenizer: True\n \n            use_keras_tokenizer: False\n \n            use_pretrained_embeddings: False\n \n            use_glove_pretrained_embeddings_weights: False\n \n            use_tfidf_as_embedding_weights: True\n \n            imbalanced_classes: True\n \n            label accuracy: {0: 56.25, 1: 65.63, 2: 89.47, 3: 87.88}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if remove_class_0:\n",
    "\n",
    "\n",
    "    # Remember that without the 0 , the other labels are reindexed\n",
    "    class_weight = {0: config['params']['data']['SVM']['class_weights_1_2_3_4']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights_1_2_3_4']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights_1_2_3_4']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights_1_2_3_4']['3']}\n",
    "\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                                data = data, \n",
    "                                param_grid = param_grid, \n",
    "                                read_type = config['params']['input_data']['read_type'], \n",
    "                                sep = config['params']['input_data']['read_type'],\n",
    "                                remove_class_0 = remove_class_0,\n",
    "                                make_all_other_classes_1 = False, \n",
    "                                running_CNN = running_CNN, \n",
    "                                running_SVM = running_SVM)\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1,\n",
    "            remove_class_0 = remove_class_0)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    # if running_CNN:\n",
    "        \n",
    "    #     train_CNN(data = data, \n",
    "    #         param_grid = param_grid,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (make_all_other_classes_1 and remove_class_0):\n",
    "\n",
    "\n",
    "    if config['params']['data']['SVM']['use_class_weights']:\n",
    "\n",
    "        class_weight = {0: config['params']['data']['SVM']['class_weights']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights']['3'],\n",
    "                    4: config['params']['data']['SVM']['class_weights']['4']}\n",
    "\n",
    "\n",
    "\n",
    "        data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                                data = data, \n",
    "                                param_grid = param_grid, \n",
    "                                read_type = config['params']['input_data']['read_type'], \n",
    "                                sep = config['params']['input_data']['read_type'],\n",
    "                                remove_class_0 = remove_class_0,\n",
    "                                make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                                running_CNN = running_CNN, \n",
    "                                running_SVM = running_SVM)\n",
    "\n",
    "            # Train and Calculate Accuracy for SVM\n",
    "        if running_SVM:\n",
    "\n",
    "            train_SVM(data = data, \n",
    "                C = C, \n",
    "                kernel = kernel, \n",
    "                degree = degree, \n",
    "                gamma = gamma, \n",
    "                class_weight = class_weight,\n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning, \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_keras_tokenizer = use_keras_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                imbalanced_classes = imbalanced_classes,\n",
    "                make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                remove_class_0 = remove_class_0)\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for CNN\n",
    "        # if running_CNN:\n",
    "            \n",
    "        #     train_CNN(data = data, \n",
    "        #         param_grid = param_grid,\n",
    "        #         sent_tokenizer = sent_tokenizer, \n",
    "        #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "        #         text_cleaning = text_cleaning, \n",
    "        #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "        #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "        #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "        #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "        #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "    else:\n",
    "\n",
    "        data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                                data = data, \n",
    "                                param_grid = param_grid, \n",
    "                                read_type = config['params']['input_data']['read_type'], \n",
    "                                sep = config['params']['input_data']['read_type'],\n",
    "                                remove_class_0 = remove_class_0,\n",
    "                                make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                                running_CNN = running_CNN, \n",
    "                                running_SVM = running_SVM)\n",
    "\n",
    "            # Train and Calculate Accuracy for SVM\n",
    "        if running_SVM:\n",
    "\n",
    "            train_SVM(data = data, \n",
    "                C = C, \n",
    "                kernel = kernel, \n",
    "                degree = degree, \n",
    "                gamma = gamma, \n",
    "                class_weight = class_weight,\n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning, \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_keras_tokenizer = use_keras_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                imbalanced_classes = imbalanced_classes,\n",
    "                make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                remove_class_0 = remove_class_0)\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for CNN\n",
    "        # if running_CNN:\n",
    "            \n",
    "        #     train_CNN(data = data, \n",
    "        #         param_grid = param_grid,\n",
    "        #         sent_tokenizer = sent_tokenizer, \n",
    "        #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "        #         text_cleaning = text_cleaning, \n",
    "        #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "        #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "        #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "        #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "        #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('class': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a859f67d4acffa6abd10af9224a6e751dd4159d06149102e129e99dcb493c1c8"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}