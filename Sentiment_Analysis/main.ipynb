{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from datetime import datetime\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from sys import path\n",
    "path.append( join( join( getcwd() , 'functions/' ) ) )\n",
    "\n",
    "from functions import preprocessing, modelling, postprocessing\n",
    "from config import ConfigDict\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# temp\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Run only for the first time#\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data_dir, data, param_grid, read_type, sep, remove_class_0, \n",
    "                make_all_other_classes_1, running_CNN, running_SVM,\n",
    "                timestamp, output_path_vectorizer, store_tfidf_tokenizer, file_path_glove,  debbug = False ):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 1. Ingest Data\n",
    "    #----------------------------------------------------------------#   \n",
    "\n",
    "    if read_type == 'excel':\n",
    "    \n",
    "        corpus = pd.read_excel(input_data, engine='openpyxl')\n",
    "\n",
    "    elif read_type == 'csv':\n",
    "\n",
    "        corpus = pd.read_csv( input_data, sep = sep)\n",
    "\n",
    "    # Filter all NAs values\n",
    "    corpus.dropna(inplace= True)\n",
    "\n",
    "    # Make Sure labels are integers\n",
    "    corpus['label'] = corpus['label'].astype(int)\n",
    "\n",
    "    # Perform data cleaning\n",
    "    corpus = preprocessing.data_cleaning(corpus = corpus,\n",
    "                        sent_tokenizer = False, \n",
    "                        text_cleaning = True, \n",
    "                        use_nltk_cleaning = False)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 2. Preprocess Data\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # Create Filter on the data to avoid the imbalance classes problem\n",
    "    if make_all_other_classes_1:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus['label'] = np.where( corpus['label'] > 0 , 1, corpus['label'])\n",
    "\n",
    "    if remove_class_0:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus = corpus[~corpus['label'].isin([0])]\n",
    "\n",
    "        # Reindex classes\n",
    "        corpus['label'] = corpus['label'].map({1:0,2:1,3:2,4:3})\n",
    "\n",
    "    print(\" The unique labels are \", corpus['label'].unique())\n",
    "\n",
    "    model_data = preprocessing.prepare_training_data(corpus)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    data = {**data, **model_data}\n",
    "\n",
    "    if debbug:\n",
    "        data['corpus'] = corpus\n",
    "        return data, param_grid\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 3. Vectorization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    if running_CNN:\n",
    "    \n",
    "        data['X_train_CNN'], data['X_test_CNN'], data['vocab_size'], data['vocab'] = modelling.keras_tokenizer(num_words = data['num_words'], \n",
    "                                                                                            sentences_train = data['sentences_train_CNN'] , \n",
    "                                                                                            sentences_test = data['sentences_test_CNN'],\n",
    "                                                                                            seq_input_len = data['seq_input_len'])\n",
    "        if use_tfidf_as_embedding_weights:\n",
    "        \n",
    "            data['embedding_matrix'], data['embedding_dim']  = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                        corpus = corpus, \n",
    "                                                                        sentences_train = data['sentences_train_CNN'])\n",
    "        \n",
    "        elif use_glove_pretrained_embeddings_weights:\n",
    "            \n",
    "            data['embedding_matrix'], data['embedding_dim'] = modelling.fit_pretrained_embedding_space_glove(embedding_dim = data['embedding_dim'], \n",
    "                                                                                filepath = data['filepath'] , \n",
    "                                                                                vocab = data['vocab'])\n",
    "\n",
    "    if running_SVM: \n",
    "\n",
    "        data['X_train_SVM'], data['X_test_SVM'], data['vocab_size'], data['vocab'] = modelling.tfidf_tokenizer(num_words = data['num_words'],\n",
    "                                                                                            corpus = corpus,\n",
    "                                                                                            sentences_train = data['sentences_train_SVM'],\n",
    "                                                                                            sentences_test = data['sentences_test_SVM'], \n",
    "                                                                                            timestamp = timestamp, \n",
    "                                                                                            output_path_vectorizer = output_path_vectorizer,\n",
    "                                                                                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                                                                                            remove_class_0 = remove_class_0, \n",
    "                                                                                            make_all_other_classes_1 = make_all_other_classes_1\n",
    "                                                                                            )\n",
    "    \n",
    "    # define default valious\n",
    "    data['embedding_matrix'], data['embedding_dim'] = [] , 0\n",
    "\n",
    "    if use_tfidf_as_embedding_weights: \n",
    "        \n",
    "        data['embedding_matrix'], data['embedding_dim'] = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                                                    corpus = data['corpus'], \n",
    "                                                                                                    sentences_train_CNN = data['X_train_CNN'])\n",
    "\n",
    "    \n",
    "    if use_glove_pretrained_embeddings_weights:  \n",
    "        \n",
    "        data['embedding_matrix'] = modelling.create_embedding_matrix(\n",
    "                             filepath = file_path_glove,\n",
    "                             word_index = data['vocab'], \n",
    "                             embedding_dim = data['embedding_dim'])\n",
    "\n",
    "    # data_pre = modelling.data_vectorization(sentences_train_CNN = data['sentences_train_CNN'], \n",
    "    #                     sentences_test_CNN = data['sentences_test_CNN'], \n",
    "    #                     sentences_train_SVM = data['sentences_train_SVM'], \n",
    "    #                     sentences_test_SVM = data['sentences_test_SVM'], \n",
    "    #                     num_words = data['num_words'], \n",
    "    #                     seq_input_len = data['seq_input_len'], \n",
    "    #                     filepath = data['filepath'],\n",
    "    #                     corpus = corpus,\n",
    "    #                     vocab = data['vocab'],\n",
    "    #                     embedding_dim = data['embedding_dim'],\n",
    "    #                     running_CNN = running_CNN, \n",
    "    #                     running_SVM = running_SVM, \n",
    "    #                     use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "    #                     use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights, \n",
    "    #                     timestamp = timestamp, \n",
    "    #                     output_path_vectorizer = output_path_vectorizer, \n",
    "    #                     store_tfidf_tokenizer = store_tfidf_tokenizer)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    #data = {**data, **data_pre}\n",
    "\n",
    "    # Add final parameters\n",
    "    param_grid['embedding_matrix'] = ([data['embedding_matrix']]) \n",
    "    param_grid['output_label'] = [data['output_label']]\n",
    "    param_grid['corpus'] = corpus\n",
    "\n",
    "    return data, param_grid\n",
    "\n",
    "\n",
    "def train_SVM(data,C, kernel, degree, gamma, class_weight,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes,\n",
    "            make_all_other_classes_1,\n",
    "            remove_class_0, \n",
    "            store_SVM_model, \n",
    "            timestamp,\n",
    "            output_path_models, \n",
    "            output_path_parameters,\n",
    "            seed):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "\n",
    "    if imbalanced_classes: \n",
    "        \n",
    "        if make_all_other_classes_1: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "\n",
    "        if remove_class_0:\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "            \n",
    "\n",
    "        if not(make_all_other_classes_1 and remove_class_0) and class_weight :\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)    \n",
    "\n",
    "        else: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                    kernel = kernel,\n",
    "                    degree = degree, \n",
    "                    gamma = gamma,\n",
    "                    )\n",
    "\n",
    "    # Fit SVM Model\n",
    "    SVM.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    if store_SVM_model:\n",
    "\n",
    "        file_name = f'SVM'\n",
    "\n",
    "        if remove_class_0:\n",
    "            file_name = f'SVM_1234'\n",
    "\n",
    "        if make_all_other_classes_1:\n",
    "            file_name = f'SVM_01'\n",
    "\n",
    "        print(\"make_all_other_classes_1: \",  make_all_other_classes_1)\n",
    "        print(\"remove_class_0 :\" , remove_class_0)\n",
    "\n",
    "        postprocessing.store_to_pickle(data = SVM, \n",
    "                        output_path = output_path_models, \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = file_name  )\n",
    "\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(data['X_test_SVM'])\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    data['test_acc'] = np.round( accuracy_score(predictions_SVM, data['Y_test_SVM'])*100 , 4)\n",
    "\n",
    "    print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, data['Y_test_SVM'])*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y = SVM.predict(data['X_train_SVM'])\n",
    "    data['conf_matrix'] = confusion_matrix(data['Y_test_SVM'], predictions_SVM)/len(predictions_SVM)\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    data['label_acc'] = postprocessing.cal_label_accuracy(data['conf_matrix'], verbose  = 1)\n",
    "\n",
    "    postprocessing.write_results_txt_SVM( output_file = output_path_parameters,  \n",
    "                      timestamp = timestamp, \n",
    "                      test_acc = data['test_acc'] , \n",
    "                      label_acc = data['label_acc'], \n",
    "                      sent_tokenizer = sent_tokenizer, \n",
    "                      use_nltk_cleaning = use_nltk_cleaning, \n",
    "                      text_cleaning = text_cleaning , \n",
    "                      use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                      use_keras_tokenizer = use_keras_tokenizer, \n",
    "                      use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                      use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                      use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                      imbalanced_classes = imbalanced_classes,\n",
    "                      make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                      remove_class_0 = remove_class_0,\n",
    "                      C = C ,\n",
    "                      kernel = kernel,\n",
    "                      degree = degree, \n",
    "                      gamma = gamma,\n",
    "                      class_weight = class_weight, \n",
    "                      seed = seed)\n",
    "\n",
    "\n",
    "\n",
    "def train_CNN(data, param_grid,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # Run CNN with Hyperparameter Optimization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    model_output = modelling.hyperparameter_optimization( \n",
    "                                    X_train = data['X_train_CNN'], \n",
    "                                    Y_train = data['Y_train_CNN'], \n",
    "                                    X_test = data['X_test_CNN'], \n",
    "                                    Y_test = data['Y_test_CNN'] , \n",
    "                                    epochs = data['epochs'] , \n",
    "                                    batch_size = data['batch_size'],\n",
    "                                    param_grid = param_grid,\n",
    "                                    cv = data['cv'], \n",
    "                                    n_iter = data['n_iter'],\n",
    "                                    verbose = False)\n",
    "\n",
    "    # 5. Score Analysis\n",
    "\n",
    "    # Generate Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(model_output['Y_pred'], data['Y_test_CNN'].argmax(axis=1)) / len(model_output['Y_pred'])\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    model_output['label_acc'] = postprocessing.cal_label_accuracy(conf_matrix, verbose  = 1)\n",
    "\n",
    "    # 6. Write Results to text file\n",
    "    postprocessing.write_results_txt_CNN(output_file = data['output_file'],\n",
    "                best_train_acc = model_output['best_train_acc'], \n",
    "                best_train_param = model_output['best_train_param'],\n",
    "                test_acc = model_output['test_acc'], \n",
    "                label_acc = model_output['label_acc'] , \n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning , \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_keras_tokenizer = use_keras_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                epochs = data['epochs'],\n",
    "                batch_size = data['batch_size'],\n",
    "                num_words = data['num_words'], \n",
    "                cv = data['cv'] ,\n",
    "                n_iter = data['n_iter']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigDict.read('config/config_param.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'params': {'model': {'running_CNN': True, 'running_SVM': True, 'seed': 123},\n",
       "  'input_data': {'data': 'data/ML_data_2.0.xlsx',\n",
       "   'sep': ',',\n",
       "   'read_type': 'excel'},\n",
       "  'output_data': {'output_path_vectorizer': 'results/vectorizer/',\n",
       "   'output_path_model': 'results/model/',\n",
       "   'output_parameters': 'results/parameters/',\n",
       "   'store_tfidf_tokenizer': True,\n",
       "   'store_keras_tokenizer': True,\n",
       "   'store_SVM_model': True,\n",
       "   'store_CNN_model': True},\n",
       "  'tokenization_options': {'sent_tokenizer': False,\n",
       "   'use_nltk_cleaning': True,\n",
       "   'text_cleaning': False,\n",
       "   'use_tfidf_tokenizer': True,\n",
       "   'use_keras_tokenizer': True,\n",
       "   'use_pretrained_embeddings': False,\n",
       "   'use_glove_pretrained_embeddings_weights': False,\n",
       "   'use_tfidf_as_embedding_weights': False,\n",
       "   'imbalanced_classes': True,\n",
       "   'make_all_other_classes_1': True,\n",
       "   'remove_class_0': True},\n",
       "  'data': {'epochs': 30,\n",
       "   'batch_size': 10,\n",
       "   'num_words': 5000,\n",
       "   'cv': 4,\n",
       "   'n_iter': 5,\n",
       "   'seq_input_len': 40,\n",
       "   'embedding_dim': 40,\n",
       "   'nodes_hidden_dense_layer': 5,\n",
       "   'filepath': 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt',\n",
       "   'SVM': {'C': 1.0,\n",
       "    'kernel': 'linear',\n",
       "    'degree': 3,\n",
       "    'gamma': 'auto',\n",
       "    'use_class_weights': True,\n",
       "    'class_weights': {'0': 0.05, '1': 1, '2': 1, '3': 1, '4': 1},\n",
       "    'class_weights_2': {'0': 0.25, '1': 1},\n",
       "    'class_weights_1_2_3_4': {'0': 1, '1': 1, '2': 1, '3': 1}}},\n",
       "  'hyperparam': {'num_filters_cv': [[64, 16], [64, 32], [128, 16], [128, 32], [256, 64], [256, 32], [256, 64], [512, 128], [512, 32]],\n",
       "   'kernel_size_cv': [[2, 3], [2, 4], [3, 4], [3, 5]],\n",
       "   'vocab_size': [3000, 4000, 5000, 6000],\n",
       "   'embedding_dim': [20, 30, 40, 50],\n",
       "   'seq_input_len': [50, 40, 30, 20, 10],\n",
       "   'nodes_hidden_dense_layer': [5, 10, 15, 20, 40],\n",
       "   'use_pretrained_embeddings': [True, False]}}}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "source": [
    "## Run Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------#\n",
    "# 0. Parameters\n",
    "# ----------------------------------------------------------------#\n",
    "\n",
    "# Define Timestamp to store models and vectorizer\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# output paths \n",
    "output_path_vectorizer = config['params']['output_data']['output_path_vectorizer']\n",
    "output_path_models = config['params']['output_data']['output_path_model']\n",
    "output_path_parameters = config['params']['output_data']['output_parameters']\n",
    "\n",
    "# Storage options\n",
    "store_tfidf_tokenizer = config['params']['output_data']['store_tfidf_tokenizer']\n",
    "store_SVM_model = config['params']['output_data']['store_SVM_model']\n",
    "\n",
    "# Set seed\n",
    "seed = config['params']['model']['seed']\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Current Date\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "\n",
    "# Input data\n",
    "input_data = config['params']['input_data']['data']\n",
    "read_type  = config['params']['input_data']['read_type']\n",
    "\n",
    "# Models to run\n",
    "running_CNN = config['params']['model']['running_CNN']\n",
    "running_SVM = config['params']['model']['running_SVM']\n",
    "\n",
    "# To test for only clases 0 and 1\n",
    "make_all_other_classes_1 =  config['params']['tokenization_options']['make_all_other_classes_1']\n",
    "remove_class_0 = config['params']['tokenization_options']['remove_class_0']\n",
    "\n",
    "\n",
    "# Sentence Tokenizer\n",
    "sent_tokenizer = config['params']['tokenization_options']['sent_tokenizer'] # TODO: Adjust for input to CNN\n",
    "\n",
    "# Text Cleaning Options\n",
    "use_nltk_cleaning = config['params']['tokenization_options']['use_nltk_cleaning']\n",
    "text_cleaning = config['params']['tokenization_options']['text_cleaning']\n",
    "\n",
    "# Word Tokenizer Options\n",
    "use_tfidf_tokenizer = config['params']['tokenization_options']['use_tfidf_tokenizer'] # For SVM\n",
    "use_keras_tokenizer = config['params']['tokenization_options']['use_keras_tokenizer'] # For CNN\n",
    "\n",
    "# If set to FALSE then keras embedding space training is used instead\n",
    "# Embedding Space possibilites are GloVe or TFIDF\n",
    "use_pretrained_embeddings = config['params']['tokenization_options']['use_pretrained_embeddings']\n",
    "\n",
    "# Only if use_pretrained_embeddings == True then select embedding vector space type\n",
    "use_glove_pretrained_embeddings_weights = config['params']['tokenization_options']['use_glove_pretrained_embeddings_weights']\n",
    "use_tfidf_as_embedding_weights = config['params']['tokenization_options']['use_tfidf_as_embedding_weights']\n",
    "\n",
    "# Options for SVM\n",
    "imbalanced_classes = config['params']['tokenization_options']['imbalanced_classes']\n",
    "C = config['params']['data']['SVM']['C']\n",
    "kernel = config['params']['data']['SVM']['kernel']\n",
    "degree = config['params']['data']['SVM']['degree']\n",
    "gamma = config['params']['data']['SVM']['gamma']\n",
    "\n",
    "# Define Class Weights as empty dict\n",
    "class_weight = {}\n",
    "\n",
    "\n",
    "# Dictionary which will cotain all the model's variables\n",
    "data = {}\n",
    "\n",
    "# Initialize Model\n",
    "data['epochs'] = config['params']['data']['epochs'] # NO. of optimization runs\n",
    "data['batch_size'] = config['params']['data']['batch_size'] # No. of sentences batch to train\n",
    "data['num_words'] = config['params']['data']['num_words'] # No. of words to use in the embedding space of GloVe or TFIDF\n",
    "data['cv'] = config['params']['data']['cv'] # No. of Cross Validations\n",
    "data['n_iter'] = config['params']['data']['n_iter'] # No. of Iterations\n",
    "data['seq_input_len'] = config['params']['data']['seq_input_len'] # Length of the vector sentence ( no. of words per sentence)\n",
    "data['embedding_dim'] = config['params']['data']['embedding_dim'] # Length of the word vector ( dimension in the embedding space)\n",
    "data['nodes_hidden_dense_layer'] = config['params']['data']['nodes_hidden_dense_layer'] # No. of nodes for hidden Dense layer\n",
    "\n",
    "\n",
    "data['filepath'] = config['params']['data']['filepath'] # File path to GLoVe pretrained embedding words\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "param_grid = dict(num_filters_cv = config['params']['hyperparam']['num_filters_cv'], # No of filter to use in convolution\n",
    "                kernel_size_cv = config['params']['hyperparam']['kernel_size_cv'], # No of words to check per Convolution \n",
    "                vocab_size = config['params']['hyperparam']['vocab_size'], # Vocab size if keras embedding space training is wanted\n",
    "                embedding_dim = config['params']['hyperparam']['embedding_dim'], \n",
    "                seq_input_len = config['params']['hyperparam']['seq_input_len'], \n",
    "                nodes_hidden_dense_layer = config['params']['hyperparam']['nodes_hidden_dense_layer'],\n",
    "                use_pretrained_embeddings = config['params']['hyperparam']['use_pretrained_embeddings']\n",
    "                )\n",
    "# Small Test\n",
    "param_grid = dict(num_filters_cv = [(64,16)],\n",
    "                    kernel_size_cv = [(2,3)],\n",
    "                    vocab_size = [5000], \n",
    "                    embedding_dim = [50],\n",
    "                    seq_input_len = [50], \n",
    "                    nodes_hidden_dense_layer = [5],\n",
    "                    use_pretrained_embeddings = [config['params']['hyperparam']['use_pretrained_embeddings']])\n"
   ]
  },
  {
   "source": [
    "## Case 1: Model for Labels 0 and make 1, 2, 3, 4 equal to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 1]\n",
      "Creating Model...\n",
      "Selecting Parameters...\n",
      "Evaluating Model...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1557",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1557",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-672a7eba92d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0muse_pretrained_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_pretrained_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0muse_glove_pretrained_embeddings_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_glove_pretrained_embeddings_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-76660aeb4c7c>\u001b[0m in \u001b[0;36mtrain_CNN\u001b[1;34m(data, param_grid, sent_tokenizer, use_nltk_cleaning, text_cleaning, use_tfidf_tokenizer, use_keras_tokenizer, use_pretrained_embeddings, use_glove_pretrained_embeddings_weights, use_tfidf_as_embedding_weights)\u001b[0m\n\u001b[0;32m    282\u001b[0m                                     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                                     \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                                     verbose = False)\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;31m# 5. Score Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Data_Science\\ClassificationProblems\\Sentiment_Analysis\\functions\\modelling.py\u001b[0m in \u001b[0;36mhyperparameter_optimization\u001b[1;34m(X_train, Y_train, X_test, Y_test, epochs, batch_size, param_grid, cv, n_iter, verbose)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evaluating Model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# Fit Selected Model with Random Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# Predict Y values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0;32m    269\u001b[0m                                                 random_state=rnd):\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_lists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1557"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "if make_all_other_classes_1:\n",
    "\n",
    "    remove_class_0_case_1 = False\n",
    "\n",
    "    class_weight = {0: config['params']['data']['SVM']['class_weights_2']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights_2']['1']}\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_1,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    # if running_SVM:\n",
    "\n",
    "    #     train_SVM(data = data, \n",
    "    #         C = C, \n",
    "    #         kernel = kernel, \n",
    "    #         degree = degree, \n",
    "    #         gamma = gamma, \n",
    "    #         class_weight = class_weight,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "    #         imbalanced_classes = imbalanced_classes,\n",
    "    #         make_all_other_classes_1 = make_all_other_classes_1,\n",
    "    #         remove_class_0 = remove_class_0_case_1, \n",
    "    #         seed = seed, \n",
    "    #         store_SVM_model = store_SVM_model,\n",
    "    #         timestamp = timestamp,\n",
    "    #         output_path_models = output_path_models, \n",
    "    #         output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    if running_CNN:\n",
    "        \n",
    "        train_CNN(data = data, \n",
    "            param_grid = param_grid,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = False, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from postprocessing import cal_label_accuracy, store_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'verbose'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7e74c4463fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'verbose'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'verbose'"
     ]
    }
   ],
   "source": [
    "data['verbose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selecting Parameters...\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn = modelling.create_model,\n",
    "                        epochs = data['epochs'], \n",
    "                        batch_size = data['batch_size'],\n",
    "                        verbose = False)\n",
    "\n",
    "print(\"Selecting Parameters...\")\n",
    "\n",
    "# Make Random Search Cross Validation\n",
    "grid = RandomizedSearchCV(estimator = model, \n",
    "                            param_distributions = param_grid,\n",
    "                            cv = data['cv'], \n",
    "                            n_iter = data['n_iter'],\n",
    "                            verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating Model...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "453",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 453",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-25048f5df733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evaluating Model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Fit Selected Model with Random Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_train_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0;32m    269\u001b[0m                                                 random_state=rnd):\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_lists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 453"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Model...\")\n",
    "# Fit Selected Model with Random Parameters\n",
    "grid_result = grid.fit(data['X_train_CNN'], data['Y_train_CNN'], verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1026,    3,   17, ...,    0,    0,    0],\n",
       "       [  31, 1028, 1029, ...,    0,    0,    0],\n",
       "       [1031,   10,  124, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 724,   64,  725, ...,    0,    0,    0],\n",
       "       [ 486, 1267, 1834, ...,    0,    0,    0],\n",
       "       [  52, 1798, 1341, ...,    0,    0,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'num_filters_cv': [(64, 16)],\n",
       " 'kernel_size_cv': [(2, 3)],\n",
       " 'vocab_size': [5000],\n",
       " 'embedding_dim': [50],\n",
       " 'seq_input_len': [50],\n",
       " 'nodes_hidden_dense_layer': [5],\n",
       " 'use_pretrained_embeddings': [True],\n",
       " 'embedding_matrix': [[]],\n",
       " 'output_label': [2],\n",
       " 'corpus':                                                    text  label  label_orignal\n",
       " 0     stable way business life many corporate purcha...      0              0\n",
       " 1     dozens companies already learned supply demand...      0              0\n",
       " 2     capabilities profitable international business...      0              0\n",
       " 3       almost every kind manufacturer answer questions      0              0\n",
       " 4         companies already responded growing pressures      0              0\n",
       " ...                                                 ...    ...            ...\n",
       " 2001  twostep process buying leverage established pr...      1              2\n",
       " 2002  main products noncritical category office supp...      1              1\n",
       " 2003  key question respect nonproduction oriented pu...      1              1\n",
       " 2004  product category travel epenses eample pooling...      0              0\n",
       " 2005  framework agreement master contract preferred ...      0              0\n",
       " \n",
       " [2006 rows x 3 columns]}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "use_pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['X_train_CNN'], \n",
    "                                    Y_train = data['Y_train_CNN'], \n",
    "                                    X_test = data['X_test_CNN'], \n",
    "                                    Y_test = data['Y_test_CNN'] , \n",
    "                                    epochs = data['epochs'] , \n",
    "                                    batch_size = data['batch_size'],\n",
    "                                    param_grid = param_grid,\n",
    "                                    cv = data['cv'], \n",
    "                                    n_iter = data['n_iter'],\n",
    "                                    verbose = False)"
   ]
  },
  {
   "source": [
    "## Case 2: Model for 1, 2, 3, 4  and exclude 0. Convert 1, 2, 3 , 4 to 0, 1, 2, 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [3 0 2 1]\nmake_all_other_classes_1:  False\nremove_class_0 : True\nSVM Accuracy Score ->  71.85185185185186\nAccuracy for label 0 :  78.26  %\nAccuracy for label 1 :  60.0  %\nAccuracy for label 2 :  80.0  %\nAccuracy for label 3 :  70.27  %\nWritting results...\nRunning SVM Modeling \n  \n            Seed : 123\n\n            Test Accuracy : 71.8519\n\n            C : 1.0\n\n            kernel : linear\n\n            degree : 3\n \n            gamma : auto\n\n            class_weight : {0: 1, 1: 1, 2: 1, 3: 1}\n\n            sent_tokenizer : False \n   \n            use_nltk_cleaning: True\n \n            text_cleaning: False\n  \n            make_all_other_classes_1: False\n  \n            remove_class_0: True \n\n            use_tfidf_tokenizer: True\n \n            use_keras_tokenizer: False\n \n            use_pretrained_embeddings: False\n \n            use_glove_pretrained_embeddings_weights: False\n \n            use_tfidf_as_embedding_weights: False\n \n            imbalanced_classes: True\n \n            label accuracy: {0: 78.26, 1: 60.0, 2: 80.0, 3: 70.27}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if remove_class_0:\n",
    "\n",
    "    make_all_other_classes_1_case_2 = False\n",
    "    \n",
    "    # Remember that without the 0 , the other labels are reindexed\n",
    "    class_weight = {0: config['params']['data']['SVM']['class_weights_1_2_3_4']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights_1_2_3_4']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights_1_2_3_4']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights_1_2_3_4']['3']}\n",
    "\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_2, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1_case_2,\n",
    "            remove_class_0 = remove_class_0, \n",
    "            seed = seed, \n",
    "            store_SVM_model = store_SVM_model,\n",
    "            timestamp = timestamp,\n",
    "            output_path_models = output_path_models, \n",
    "            output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    # if running_CNN:\n",
    "        \n",
    "    #     train_CNN(data = data, \n",
    "    #         param_grid = param_grid,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n"
   ]
  },
  {
   "source": [
    "## Case 3: Model for all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 4 1 3 2]\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : False\n",
      "SVM Accuracy Score ->  63.745019920318725\n",
      "Accuracy for label 0 :  58.97  %\n",
      "Accuracy for label 1 :  57.69  %\n",
      "Accuracy for label 2 :  63.64  %\n",
      "Accuracy for label 3 :  92.86  %\n",
      "Accuracy for label 4 :  84.85  %\n",
      "Writting results...\n",
      "Running SVM Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 63.745\n",
      "\n",
      "            C : 1.0\n",
      "\n",
      "            kernel : linear\n",
      "\n",
      "            degree : 3\n",
      " \n",
      "            gamma : auto\n",
      "\n",
      "            class_weight : {0: 0.05, 1: 1, 2: 1, 3: 1, 4: 1}\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_keras_tokenizer: False\n",
      " \n",
      "            use_pretrained_embeddings: False\n",
      " \n",
      "            use_glove_pretrained_embeddings_weights: False\n",
      " \n",
      "            use_tfidf_as_embedding_weights: False\n",
      " \n",
      "            imbalanced_classes: True\n",
      " \n",
      "            label accuracy: {0: 58.97, 1: 57.69, 2: 63.64, 3: 92.86, 4: 84.85}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    remove_class_0_case_3 = False\n",
    "    make_all_other_classes_1_case_3 = False\n",
    "\n",
    "    if config['params']['data']['SVM']['use_class_weights']:\n",
    "\n",
    "        class_weight = {0: config['params']['data']['SVM']['class_weights']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights']['3'],\n",
    "                    4: config['params']['data']['SVM']['class_weights']['4']}\n",
    "\n",
    "\n",
    "\n",
    "        data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_3,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_3, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "            # Train and Calculate Accuracy for SVM\n",
    "        if running_SVM:\n",
    "\n",
    "            train_SVM(data = data, \n",
    "                    C = C, \n",
    "                    kernel = kernel, \n",
    "                    degree = degree, \n",
    "                    gamma = gamma, \n",
    "                    class_weight = class_weight,\n",
    "                    sent_tokenizer = sent_tokenizer, \n",
    "                    use_nltk_cleaning = use_nltk_cleaning, \n",
    "                    text_cleaning = text_cleaning, \n",
    "                    use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                    use_keras_tokenizer = use_keras_tokenizer, \n",
    "                    use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                    use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                    use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                    imbalanced_classes = imbalanced_classes,\n",
    "                    make_all_other_classes_1 = make_all_other_classes_1_case_3,\n",
    "                    remove_class_0 = remove_class_0_case_3, \n",
    "                    seed = seed, \n",
    "                    store_SVM_model = store_SVM_model,\n",
    "                    timestamp = timestamp,\n",
    "                    output_path_models = output_path_models, \n",
    "                    output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for CNN\n",
    "        # if running_CNN:\n",
    "            \n",
    "        #     train_CNN(data = data, \n",
    "        #         param_grid = param_grid,\n",
    "        #         sent_tokenizer = sent_tokenizer, \n",
    "        #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "        #         text_cleaning = text_cleaning, \n",
    "        #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "        #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "        #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "        #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "        #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  raw materials purchase is decisive for the com...      1\n",
       "1      framework agreement master contract preferred      0\n",
       "2  twostep process                               ...      2\n",
       "3  companys top management promptly ordered chang...      3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>raw materials purchase is decisive for the com...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>framework agreement master contract preferred</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>twostep process                               ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>companys top management promptly ordered chang...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "new_sentence  = pd.DataFrame({'text':[\"raw materials purchase is decisive for the company\", 'framework agreement master contract preferred', 'twostep process                                        buying leverage established provoking leverage portfolio matrix' , 'companys top management promptly ordered change purchasing policy build alternative domestic sources'],\n",
    "                            'label':[1,0,2,3]})\n",
    "\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-01-17_22-37-56'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Test Case 1\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_01') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 1, 1, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Test Case 2\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer_1234')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_1234') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Test Case 3\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('class': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a859f67d4acffa6abd10af9224a6e751dd4159d06149102e129e99dcb493c1c8"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}