{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from datetime import datetime\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sys import path\n",
    "path.append( join( getcwd(), 'functions' ) )\n",
    "\n",
    "from functions import preprocessing, modelling, postprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "\n",
    "sent_tokenizer = False\n",
    "use_nltk_cleaning = False\n",
    "\n",
    "text_cleaning = True\n",
    "\n",
    "use_tfidf_tokenizer = False # TODO: Adjust for input to CNN\n",
    "use_keras_tokenizer = True\n",
    "\n",
    "use_glove_pretrained_embeddings_weights = False\n",
    "use_tfidf_as_embedding_weights = False\n",
    "use_pretrained_embeddings = True\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Initialize Model\n",
    "data['epochs'] = 30\n",
    "data['batch_size'] = 16\n",
    "data['num_words'] = 5000\n",
    "data['cv'] = 4\n",
    "data['n_iter'] = 5\n",
    "data['seq_input_len'] = 40\n",
    "data['embedding_dim'] = 40\n",
    "data['nodes_hidden_dense_layer'] = 5\n",
    "\n",
    "\n",
    "data['filepath'] = 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt'\n",
    "data['output_file'] = f\"results/{current_time}_Result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(num_filters_cv = [(64,16), (64,32), (128,16), (128,32), (256,64), (256,32), (256,64), (512,128), (512, 32)],\n",
    "                  kernel_size_cv = [(2,3), (2,5), (2,4), (3,5), (3,7), (5,7)],\n",
    "                  vocab_size = [3000, 4000, 5000, 600], \n",
    "                  embedding_dim = [20, 30, 40, 50], \n",
    "                  seq_input_len = [50, 40, 30, 20, 10], \n",
    "                  nodes_hidden_dense_layer = [5, 10, 15, 20, 40],\n",
    "                  use_pretrained_embeddings = [True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Test\n",
    "# param_grid = dict(num_filters_cv = [(64,16)],\n",
    "#                   kernel_size_cv = [(2,3)],\n",
    "#                   vocab_size = [5000], \n",
    "#                   embedding_dim = [50],\n",
    "#                   seq_input_len = [50], \n",
    "#                   nodes_hidden_dense_layer = [5],\n",
    "#                   use_pretrained_embeddings = [True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/Data_Science/ClassificationProblems/Sentiment_Analysis/data/SA_4_Categories.csv'\n",
    "corpus = pd.read_csv(data_dir ,encoding='latin-1', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing.data_cleaning(corpus = corpus,\n",
    "                       sent_tokenizer = False, \n",
    "                       text_cleaning = True, \n",
    "                       use_nltk_cleaning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stable way business life many corporate purcha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dozens companies already learned supply demand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capabilities profitable international business...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>almost every kind manufacturer answer questions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>companies already responded growing pressures</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  stable way business life many corporate purcha...      0\n",
       "1  dozens companies already learned supply demand...      0\n",
       "2  capabilities profitable international business...      0\n",
       "3    almost every kind manufacturer answer questions      0\n",
       "4      companies already responded growing pressures      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = preprocessing.prepare_training_data(corpus)\n",
    "\n",
    "# Concat two dictionaries\n",
    "data = {**data, **model_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epochs', 'batch_size', 'num_words', 'cv', 'n_iter', 'seq_input_len', 'embedding_dim', 'nodes_hidden_dense_layer', 'filepath', 'output_file', 'sentences_train', 'sentences_test', 'Y_train', 'Y_test', 'output_label', 'X_train', 'X_test', 'vocab_size', 'vocab'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_keras_tokenizer:\n",
    "    data['X_train'], data['X_test'], data['vocab_size'], data['vocab'] = modelling.keras_tokenizer(num_words = data['num_words'], \n",
    "                                                                                         sentences_train = data['sentences_train'] , \n",
    "                                                                                         sentences_test = data['sentences_test'],\n",
    "                                                                                         seq_input_len = data['seq_input_len'])\n",
    "elif use_tfidf_tokenizer: # Not implemented yet\n",
    "    data['X_train'], data['X_test'], data['vocab_size'], data['vocab'] = modelling.tfidf_tokenizer(num_words = data['num_words'],\n",
    "                                                                                         corpus = corpus,\n",
    "                                                                                         sentences_train = data['sentences_train'],\n",
    "                                                                                         sentences_test = data['sentences_test'])\n",
    "    \n",
    "if use_tfidf_as_embedding_weights:\n",
    "    \n",
    "    data['embedding_matrix'], data['embedding_dim']  = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                  corpus = corpus, \n",
    "                                                                  sentences_train = data['sentences_train'])\n",
    "    \n",
    "elif use_glove_pretrained_embeddings_weights:\n",
    "    \n",
    "    data['embedding_matrix'], data['embedding_dim'] = modelling.fit_pretrained_embedding_space_glove(embedding_dim = data['embedding_dim'], \n",
    "                                                                           filepath = data['filepath'] , \n",
    "                                                                           vocab = data['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epochs', 'batch_size', 'num_words', 'cv', 'n_iter', 'seq_input_len', 'embedding_dim', 'nodes_hidden_dense_layer', 'filepath', 'output_file', 'sentences_train', 'sentences_test', 'Y_train', 'Y_test', 'output_label', 'X_train', 'X_test', 'vocab_size', 'vocab'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embedding_dim'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = modelling.data_vectorization(sentences_train = data['sentences_train'], \n",
    "                       sentences_test = data['sentences_test'], \n",
    "                       num_words = data['num_words'], \n",
    "                       seq_input_len = data['seq_input_len'], \n",
    "                       filepath = data['filepath'],\n",
    "                       corpus = corpus,\n",
    "                       vocab = data['vocab'],\n",
    "                       embedding_dim = data['embedding_dim'],\n",
    "                       use_keras_tokenizer = True, \n",
    "                       use_tfidf_tokenizer = False, \n",
    "                       use_tfidf_as_embedding_weights = False,\n",
    "                       use_glove_pretrained_embeddings_weights = True)\n",
    "\n",
    "# Concat two dictionaries\n",
    "data = {**data, **data_pre}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epochs', 'batch_size', 'num_words', 'cv', 'n_iter', 'seq_input_len', 'embedding_dim', 'nodes_hidden_dense_layer', 'filepath', 'output_file', 'sentences_train', 'sentences_test', 'Y_train', 'Y_test', 'output_label', 'X_train', 'X_test', 'vocab_size', 'vocab', 'embedding_matrix'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid['embedding_matrix'] = ([data['embedding_matrix']])\n",
    "param_grid['output_label'] = [data['output_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Selecting Parameters...\n",
      "Evaluating Model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 40)            117560    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 39, 256)           20736     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 36, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 204,251\n",
      "Trainable params: 86,691\n",
      "Non-trainable params: 117,560\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 40)            117560    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 39, 256)           20736     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 36, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 204,251\n",
      "Trainable params: 86,691\n",
      "Non-trainable params: 117,560\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 40)            117560    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 39, 256)           20736     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 36, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 204,251\n",
      "Trainable params: 86,691\n",
      "Non-trainable params: 117,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " label accuracy could not be calculated\n",
      "Writting results...\n",
      "Running CNN Modeling data set\n",
      "Best Accuracy : 0.7968\n",
      "{'vocab_size': 3000, 'use_pretrained_embeddings': True, 'seq_input_len': 40, 'output_label': 5, 'num_filters_cv': (256, 64), 'nodes_hidden_dense_layer': 5, 'kernel_size_cv': (2, 4), 'embedding_matrix': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [-0.14896999, -0.41446   ,  0.57534999, ...,  0.025145  ,\n",
      "         0.32242   , -0.053632  ],\n",
      "       [-0.097028  , -0.44775   , -0.25226   , ...,  0.71882999,\n",
      "        -0.39734   ,  0.12426   ],\n",
      "       ...,\n",
      "       [ 0.76950002,  0.38067999, -0.32255   , ...,  0.20100001,\n",
      "        -0.38712999,  1.02090001],\n",
      "       [-0.039673  , -0.021234  , -0.43268001, ..., -0.080242  ,\n",
      "        -0.92989999, -0.18745001],\n",
      "       [ 0.77649999, -0.70692003, -0.24706   , ...,  0.77344   ,\n",
      "        -0.55791003, -0.21297   ]]), 'embedding_dim': 50}\n",
      "Test Accuracy : 0.8065\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = modelling.hyperparameter_optimization( \n",
    "                            X_train = data['X_train'], \n",
    "                            Y_train = data['Y_train'], \n",
    "                            X_test = data['X_test'], \n",
    "                            Y_test = data['Y_test'] , \n",
    "                            epochs = data['epochs'] , \n",
    "                            batch_size = data['batch_size'],\n",
    "                            param_grid = param_grid,\n",
    "                            cv = 2, #data['cv'], \n",
    "                            n_iter = 1, #data['n_iter'],\n",
    "                            output_file = data['output_file'],\n",
    "                            verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for label 0 :  88.09  %\n",
      "Accuracy for label 1 :  66.67  %\n",
      "Accuracy for label 2 :  44.0  %\n",
      "Accuracy for label 3 :  50.0  %\n",
      "Accuracy for label 4 :  59.26  %\n"
     ]
    }
   ],
   "source": [
    "# Generate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_pred, data['Y_test'].argmax(axis=1)) / len(Y_pred)\n",
    "\n",
    "# Calculate Label Accuracy\n",
    "label_acc = postprocessing.cal_label_accuracy(conf_matrix, verbose  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
