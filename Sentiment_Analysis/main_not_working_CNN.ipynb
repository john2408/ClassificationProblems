{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from datetime import datetime\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from sys import path\n",
    "path.append( join( join( getcwd() , 'functions/' ) ) )\n",
    "\n",
    "from functions import preprocessing, modelling, postprocessing\n",
    "from config import ConfigDict\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# temp\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Run only for the first time#\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data_dir, data, param_grid, read_type, sep, remove_class_0, \n",
    "                make_all_other_classes_1, running_CNN, running_SVM,\n",
    "                timestamp, output_path_vectorizer, store_tfidf_tokenizer, \n",
    "                store_keras_tokenizer, file_path_glove,  debbug = False ):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 1. Ingest Data\n",
    "    #----------------------------------------------------------------#   \n",
    "\n",
    "    if read_type == 'excel':\n",
    "    \n",
    "        corpus = pd.read_excel(input_data, engine='openpyxl')\n",
    "\n",
    "    elif read_type == 'csv':\n",
    "\n",
    "        corpus = pd.read_csv( input_data, sep = sep)\n",
    "\n",
    "    # Filter all NAs values\n",
    "    corpus.dropna(inplace= True)\n",
    "\n",
    "    # Make Sure labels are integers\n",
    "    corpus['label'] = corpus['label'].astype(int)\n",
    "\n",
    "    # Perform data cleaning\n",
    "    corpus = preprocessing.data_cleaning(corpus = corpus,\n",
    "                        sent_tokenizer = False, \n",
    "                        text_cleaning = True, \n",
    "                        use_nltk_cleaning = False)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 2. Preprocess Data\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # Create Filter on the data to avoid the imbalance classes problem\n",
    "    if make_all_other_classes_1:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus['label'] = np.where( corpus['label'] > 0 , 1, corpus['label'])\n",
    "\n",
    "    if remove_class_0:\n",
    "\n",
    "        corpus['label_orignal'] = corpus.loc[:,'label']\n",
    "        corpus = corpus[~corpus['label'].isin([0])]\n",
    "\n",
    "        # Reindex classes\n",
    "        corpus['label'] = corpus['label'].map({1:0,2:1,3:2,4:3})\n",
    "\n",
    "    print(\" The unique labels are \", corpus['label'].unique())\n",
    "\n",
    "    model_data = preprocessing.prepare_training_data(corpus)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    data = {**data, **model_data}\n",
    "\n",
    "    if debbug:\n",
    "        data['corpus'] = corpus\n",
    "        return data, param_grid\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # 3. Vectorization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    if running_CNN:\n",
    "    \n",
    "        data['X_train_CNN'], data['X_test_CNN'], data['vocab_size'], data['vocab'] = modelling.keras_tokenizer(num_words = data['num_words'], \n",
    "                                                                                            sentences_train = data['sentences_train_CNN'] , \n",
    "                                                                                            sentences_test = data['sentences_test_CNN'],\n",
    "                                                                                            seq_input_len = data['seq_input_len'],\n",
    "                                                                                            store_keras_tokenizer= store_keras_tokenizer, \n",
    "                                                                                            remove_class_0 = remove_class_0, \n",
    "                                                                                            make_all_other_classes_1 =make_all_other_classes_1, \n",
    "                                                                                            output_path_vectorizer = output_path_vectorizer, \n",
    "                                                                                            timestamp = timestamp)\n",
    "        if use_tfidf_as_embedding_weights:\n",
    "        \n",
    "            data['embedding_matrix'], data['embedding_dim']  = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                        corpus = corpus, \n",
    "                                                                        sentences_train = data['sentences_train_CNN'])\n",
    "        \n",
    "        elif use_glove_pretrained_embeddings_weights:\n",
    "            \n",
    "            data['embedding_matrix'], data['embedding_dim'] = modelling.fit_pretrained_embedding_space_glove(embedding_dim = data['embedding_dim'], \n",
    "                                                                                filepath = data['filepath'] , \n",
    "                                                                                vocab = data['vocab'])\n",
    "\n",
    "    if running_SVM: \n",
    "\n",
    "        data['X_train_SVM'], data['X_test_SVM'], data['vocab_size'], data['vocab'] = modelling.tfidf_tokenizer(num_words = data['num_words'],\n",
    "                                                                                            corpus = corpus,\n",
    "                                                                                            sentences_train = data['sentences_train_SVM'],\n",
    "                                                                                            sentences_test = data['sentences_test_SVM'], \n",
    "                                                                                            timestamp = timestamp, \n",
    "                                                                                            output_path_vectorizer = output_path_vectorizer,\n",
    "                                                                                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                                                                                            remove_class_0 = remove_class_0, \n",
    "                                                                                            make_all_other_classes_1 = make_all_other_classes_1\n",
    "                                                                                            )\n",
    "\n",
    "    if use_tfidf_as_embedding_weights: \n",
    "        \n",
    "        data['embedding_matrix'], data['embedding_dim'] = modelling.tfidf_as_embedding_weights(num_words = data['num_words'], \n",
    "                                                                                                    corpus = data['corpus'], \n",
    "                                                                                                    sentences_train_CNN = data['X_train_CNN'])\n",
    "\n",
    "    \n",
    "    if use_glove_pretrained_embeddings_weights:  \n",
    "        \n",
    "        data['embedding_matrix'] = modelling.create_embedding_matrix(\n",
    "                             filepath = file_path_glove,\n",
    "                             word_index = data['vocab'], \n",
    "                             embedding_dim = data['embedding_dim'])\n",
    "\n",
    "    # data_pre = modelling.data_vectorization(sentences_train_CNN = data['sentences_train_CNN'], \n",
    "    #                     sentences_test_CNN = data['sentences_test_CNN'], \n",
    "    #                     sentences_train_SVM = data['sentences_train_SVM'], \n",
    "    #                     sentences_test_SVM = data['sentences_test_SVM'], \n",
    "    #                     num_words = data['num_words'], \n",
    "    #                     seq_input_len = data['seq_input_len'], \n",
    "    #                     filepath = data['filepath'],\n",
    "    #                     corpus = corpus,\n",
    "    #                     vocab = data['vocab'],\n",
    "    #                     embedding_dim = data['embedding_dim'],\n",
    "    #                     running_CNN = running_CNN, \n",
    "    #                     running_SVM = running_SVM, \n",
    "    #                     use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "    #                     use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights, \n",
    "    #                     timestamp = timestamp, \n",
    "    #                     output_path_vectorizer = output_path_vectorizer, \n",
    "    #                     store_tfidf_tokenizer = store_tfidf_tokenizer)\n",
    "\n",
    "    # Concat two dictionaries\n",
    "    #data = {**data, **data_pre}\n",
    "\n",
    "    # Add final parameters\n",
    "    param_grid['embedding_matrix'] = ([data['embedding_matrix']]) \n",
    "    param_grid['output_label'] = [data['output_label']]\n",
    "    param_grid['corpus'] = corpus\n",
    "\n",
    "    return data, param_grid\n",
    "\n",
    "\n",
    "def train_SVM(data,C, kernel, degree, gamma, class_weight,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes,\n",
    "            make_all_other_classes_1,\n",
    "            remove_class_0, \n",
    "            store_SVM_model, \n",
    "            timestamp,\n",
    "            output_path_models, \n",
    "            output_path_parameters,\n",
    "            seed):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "\n",
    "    if imbalanced_classes: \n",
    "        \n",
    "        if make_all_other_classes_1: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "\n",
    "        if remove_class_0:\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)\n",
    "\n",
    "            \n",
    "\n",
    "        if not(make_all_other_classes_1 and remove_class_0) and class_weight :\n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                kernel = kernel,\n",
    "                degree = degree, \n",
    "                gamma = gamma,\n",
    "                class_weight = class_weight)    \n",
    "\n",
    "        else: \n",
    "\n",
    "            SVM = svm.SVC(C = C, \n",
    "                    kernel = kernel,\n",
    "                    degree = degree, \n",
    "                    gamma = gamma,\n",
    "                    )\n",
    "\n",
    "    # Fit SVM Model\n",
    "    SVM.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    if store_SVM_model:\n",
    "\n",
    "        file_name = f'SVM'\n",
    "\n",
    "        if remove_class_0:\n",
    "            file_name = f'SVM_1234'\n",
    "\n",
    "        if make_all_other_classes_1:\n",
    "            file_name = f'SVM_01'\n",
    "\n",
    "        print(\"make_all_other_classes_1: \",  make_all_other_classes_1)\n",
    "        print(\"remove_class_0 :\" , remove_class_0)\n",
    "\n",
    "        postprocessing.store_to_pickle(data = SVM, \n",
    "                        output_path = output_path_models, \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = file_name  )\n",
    "\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(data['X_test_SVM'])\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    data['test_acc'] = np.round( accuracy_score(predictions_SVM, data['Y_test_SVM'])*100 , 4)\n",
    "\n",
    "    print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, data['Y_test_SVM'])*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y = SVM.predict(data['X_train_SVM'])\n",
    "    data['conf_matrix'] = confusion_matrix(data['Y_test_SVM'], predictions_SVM)/len(predictions_SVM)\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    data['label_acc'] = postprocessing.cal_label_accuracy(data['conf_matrix'], verbose  = 1)\n",
    "\n",
    "    postprocessing.write_results_txt_SVM( output_file = output_path_parameters,  \n",
    "                      timestamp = timestamp, \n",
    "                      test_acc = data['test_acc'] , \n",
    "                      label_acc = data['label_acc'], \n",
    "                      sent_tokenizer = sent_tokenizer, \n",
    "                      use_nltk_cleaning = use_nltk_cleaning, \n",
    "                      text_cleaning = text_cleaning , \n",
    "                      use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                      use_keras_tokenizer = use_keras_tokenizer, \n",
    "                      use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                      use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                      use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                      imbalanced_classes = imbalanced_classes,\n",
    "                      make_all_other_classes_1 = make_all_other_classes_1,\n",
    "                      remove_class_0 = remove_class_0,\n",
    "                      C = C ,\n",
    "                      kernel = kernel,\n",
    "                      degree = degree, \n",
    "                      gamma = gamma,\n",
    "                      class_weight = class_weight, \n",
    "                      seed = seed)\n",
    "\n",
    "\n",
    "\n",
    "def train_CNN(data, param_grid,\n",
    "            sent_tokenizer, \n",
    "            use_nltk_cleaning, \n",
    "            text_cleaning, \n",
    "            use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer, \n",
    "            use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights):\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # Run CNN with Hyperparameter Optimization\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    model_output = modelling.hyperparameter_optimization( \n",
    "                                    X_train = data['X_train_CNN'], \n",
    "                                    Y_train = data['Y_train_CNN'], \n",
    "                                    X_test = data['X_test_CNN'], \n",
    "                                    Y_test = data['Y_test_CNN'] , \n",
    "                                    epochs = data['epochs'] , \n",
    "                                    batch_size = data['batch_size'],\n",
    "                                    param_grid = param_grid,\n",
    "                                    cv = data['cv'], \n",
    "                                    n_iter = data['n_iter'],\n",
    "                                    verbose = False)\n",
    "\n",
    "    # 5. Score Analysis\n",
    "\n",
    "    # Generate Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(model_output['Y_pred'], data['Y_test_CNN'].argmax(axis=1)) / len(model_output['Y_pred'])\n",
    "\n",
    "    # Calculate Label Accuracy\n",
    "    model_output['label_acc'] = postprocessing.cal_label_accuracy(conf_matrix, verbose  = 1)\n",
    "\n",
    "    # 6. Write Results to text file\n",
    "    postprocessing.write_results_txt_CNN(output_file = data['output_file'],\n",
    "                best_train_acc = model_output['best_train_acc'], \n",
    "                best_train_param = model_output['best_train_param'],\n",
    "                test_acc = model_output['test_acc'], \n",
    "                label_acc = model_output['label_acc'] , \n",
    "                sent_tokenizer = sent_tokenizer, \n",
    "                use_nltk_cleaning = use_nltk_cleaning, \n",
    "                text_cleaning = text_cleaning , \n",
    "                use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                use_keras_tokenizer = use_keras_tokenizer, \n",
    "                use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                epochs = data['epochs'],\n",
    "                batch_size = data['batch_size'],\n",
    "                num_words = data['num_words'], \n",
    "                cv = data['cv'] ,\n",
    "                n_iter = data['n_iter']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def train_bayes(data):\n",
    "\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(data['X_train_SVM'], data['Y_train_SVM'])\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "\n",
    "    # Print Confusion Matrix\n",
    "    Pred_Y = Naive.predict(Test_X_Tfidf)\n",
    "    confusion_matrix(Test_Y, Pred_Y)/len(Pred_Y)\n"
   ]
  },
  {
   "source": [
    "## Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigDict.read('config/config_param.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'params': {'model': {'running_CNN': True, 'running_SVM': True, 'seed': 123},\n",
       "  'input_data': {'data': 'data/ML_data_2.0.xlsx',\n",
       "   'sep': ',',\n",
       "   'read_type': 'excel'},\n",
       "  'output_data': {'output_path_vectorizer': 'results/vectorizer/',\n",
       "   'output_path_model': 'results/model/',\n",
       "   'output_parameters': 'results/parameters/',\n",
       "   'store_tfidf_tokenizer': True,\n",
       "   'store_keras_tokenizer': True,\n",
       "   'store_SVM_model': True,\n",
       "   'store_CNN_model': True},\n",
       "  'tokenization_options': {'sent_tokenizer': False,\n",
       "   'use_nltk_cleaning': True,\n",
       "   'text_cleaning': False,\n",
       "   'use_tfidf_tokenizer': True,\n",
       "   'use_keras_tokenizer': True,\n",
       "   'use_pretrained_embeddings': False,\n",
       "   'use_glove_pretrained_embeddings_weights': True,\n",
       "   'use_tfidf_as_embedding_weights': False,\n",
       "   'imbalanced_classes': True,\n",
       "   'make_all_other_classes_1': True,\n",
       "   'remove_class_0': True},\n",
       "  'CNN': {'epochs': 30,\n",
       "   'batch_size': 10,\n",
       "   'num_words': 5000,\n",
       "   'cv': 4,\n",
       "   'n_iter': 5,\n",
       "   'seq_input_len': 40,\n",
       "   'embedding_dim': 40,\n",
       "   'nodes_hidden_dense_layer': 5,\n",
       "   'filepath_GloVe': 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt',\n",
       "   'grid_search': {'num_filters_cv': [[64, 16], [64, 32], [128, 16], [128, 32], [256, 64], [256, 32], [256, 64], [512, 128], [512, 32]],\n",
       "    'kernel_size_cv': [[2, 3], [2, 4], [3, 4], [3, 5]],\n",
       "    'vocab_size': [3000, 4000, 5000, 6000],\n",
       "    'embedding_dim': [20, 30, 40, 50],\n",
       "    'seq_input_len': [50, 40, 30, 20, 10],\n",
       "    'nodes_hidden_dense_layer': [5, 10, 15, 20, 40],\n",
       "    'use_pretrained_embeddings': [True, False]}},\n",
       "  'SVM': {'C': 1.0,\n",
       "   'kernel': 'linear',\n",
       "   'degree': 3,\n",
       "   'gamma': 'auto',\n",
       "   'use_class_weights': True,\n",
       "   'class_weights': {'0': 0.05, '1': 1, '2': 1, '3': 1, '4': 1},\n",
       "   'class_weights_2': {'0': 0.25, '1': 1},\n",
       "   'class_weights_1_2_3_4': {'0': 1, '1': 1, '2': 1, '3': 1}}}}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "source": [
    "## Run Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------#\n",
    "# 0. Parameters\n",
    "# ----------------------------------------------------------------#\n",
    "\n",
    "# Define Timestamp to store models and vectorizer\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# output paths \n",
    "output_path_vectorizer = config['params']['output_data']['output_path_vectorizer']\n",
    "output_path_models = config['params']['output_data']['output_path_model']\n",
    "output_path_parameters = config['params']['output_data']['output_parameters']\n",
    "\n",
    "# Storage options\n",
    "store_tfidf_tokenizer = config['params']['output_data']['store_tfidf_tokenizer']\n",
    "store_keras_tokenizer = config['params']['output_data']['store_keras_tokenizer']\n",
    "store_SVM_model = config['params']['output_data']['store_SVM_model']\n",
    "\n",
    "# Set seed\n",
    "seed = config['params']['model']['seed']\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Current Date\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "\n",
    "# Input data\n",
    "input_data = config['params']['input_data']['data']\n",
    "read_type  = config['params']['input_data']['read_type']\n",
    "\n",
    "# Models to run\n",
    "running_CNN = config['params']['model']['running_CNN']\n",
    "running_SVM = config['params']['model']['running_SVM']\n",
    "\n",
    "# To test for only clases 0 and 1\n",
    "make_all_other_classes_1 =  config['params']['tokenization_options']['make_all_other_classes_1']\n",
    "remove_class_0 = config['params']['tokenization_options']['remove_class_0']\n",
    "\n",
    "\n",
    "# Sentence Tokenizer\n",
    "sent_tokenizer = config['params']['tokenization_options']['sent_tokenizer'] # TODO: Adjust for input to CNN\n",
    "\n",
    "# Text Cleaning Options\n",
    "use_nltk_cleaning = config['params']['tokenization_options']['use_nltk_cleaning']\n",
    "text_cleaning = config['params']['tokenization_options']['text_cleaning']\n",
    "\n",
    "# Word Tokenizer Options\n",
    "use_tfidf_tokenizer = config['params']['tokenization_options']['use_tfidf_tokenizer'] # For SVM\n",
    "use_keras_tokenizer = config['params']['tokenization_options']['use_keras_tokenizer'] # For CNN\n",
    "\n",
    "# If set to FALSE then keras embedding space training is used instead\n",
    "# Embedding Space possibilites are GloVe or TFIDF\n",
    "use_pretrained_embeddings = config['params']['tokenization_options']['use_pretrained_embeddings']\n",
    "\n",
    "# Only if use_pretrained_embeddings == True then select embedding vector space type\n",
    "use_glove_pretrained_embeddings_weights = config['params']['tokenization_options']['use_glove_pretrained_embeddings_weights']\n",
    "use_tfidf_as_embedding_weights = config['params']['tokenization_options']['use_tfidf_as_embedding_weights']\n",
    "\n",
    "# Options for SVM\n",
    "imbalanced_classes = config['params']['tokenization_options']['imbalanced_classes']\n",
    "C = config['params']['SVM']['C']\n",
    "kernel = config['params']['SVM']['kernel']\n",
    "degree = config['params']['SVM']['degree']\n",
    "gamma = config['params']['SVM']['gamma']\n",
    "\n",
    "# Define Class Weights as empty dict\n",
    "class_weight = {}\n",
    "\n",
    "\n",
    "# Dictionary which will cotain all the model's variables\n",
    "data = {}\n",
    "\n",
    "# Initialize Model\n",
    "data['epochs'] = config['params']['CNN']['epochs'] # NO. of optimization runs\n",
    "data['batch_size'] = config['params']['CNN']['batch_size'] # No. of sentences batch to train\n",
    "data['num_words'] = config['params']['CNN']['num_words'] # No. of words to use in the embedding space of GloVe or TFIDF\n",
    "data['cv'] = config['params']['CNN']['cv'] # No. of Cross Validations\n",
    "data['n_iter'] = config['params']['CNN']['n_iter'] # No. of Iterations\n",
    "data['seq_input_len'] = config['params']['CNN']['seq_input_len'] # Length of the vector sentence ( no. of words per sentence)\n",
    "data['embedding_dim'] = config['params']['CNN']['embedding_dim'] # Length of the word vector ( dimension in the embedding space)\n",
    "data['nodes_hidden_dense_layer'] = config['params']['CNN']['nodes_hidden_dense_layer'] # No. of nodes for hidden Dense layer\n",
    "\n",
    "\n",
    "data['filepath'] = config['params']['CNN']['filepath_GloVe'] # File path to GLoVe pretrained embedding words\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "param_grid = dict(num_filters_cv = config['params']['CNN']['grid_search']['num_filters_cv'], # No of filter to use in convolution\n",
    "                kernel_size_cv = config['params']['CNN']['grid_search']['kernel_size_cv'], # No of words to check per Convolution \n",
    "                vocab_size = config['params']['CNN']['grid_search']['vocab_size'], # Vocab size if keras embedding space training is wanted\n",
    "                embedding_dim = config['params']['CNN']['grid_search']['embedding_dim'], \n",
    "                seq_input_len = config['params']['CNN']['grid_search']['seq_input_len'], \n",
    "                nodes_hidden_dense_layer = config['params']['CNN']['grid_search']['nodes_hidden_dense_layer'],\n",
    "                use_pretrained_embeddings = config['params']['CNN']['grid_search']['use_pretrained_embeddings']\n",
    "                )\n",
    "# Small Test\n",
    "param_grid = dict(num_filters_cv = [(64,16)],\n",
    "                    kernel_size_cv = [(2,3)],\n",
    "                    vocab_size = [5000], \n",
    "                    embedding_dim = [50],\n",
    "                    seq_input_len = [50], \n",
    "                    nodes_hidden_dense_layer = [5],\n",
    "                    use_pretrained_embeddings = [config['params']['CNN']['grid_search']['use_pretrained_embeddings']])\n"
   ]
  },
  {
   "source": [
    "## Case 1: Model for Labels 0 and make 1, 2, 3, 4 equal to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'epochs': 30,\n",
       " 'batch_size': 10,\n",
       " 'num_words': 5000,\n",
       " 'cv': 4,\n",
       " 'n_iter': 5,\n",
       " 'seq_input_len': 40,\n",
       " 'embedding_dim': 40,\n",
       " 'nodes_hidden_dense_layer': 5,\n",
       " 'filepath_GloVe': 'D:/Semillero Data Science/Deep Learning/pre-trained Word Embeddings/GloVe/glove.6B.50d.txt',\n",
       " 'grid_search': {'num_filters_cv': [[64, 16], [64, 32], [128, 16], [128, 32], [256, 64], [256, 32], [256, 64], [512, 128], [512, 32]],\n",
       "  'kernel_size_cv': [[2, 3], [2, 4], [3, 4], [3, 5]],\n",
       "  'vocab_size': [3000, 4000, 5000, 6000],\n",
       "  'embedding_dim': [20, 30, 40, 50],\n",
       "  'seq_input_len': [50, 40, 30, 20, 10],\n",
       "  'nodes_hidden_dense_layer': [5, 10, 15, 20, 40],\n",
       "  'use_pretrained_embeddings': [True, False]}}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "config['params']['CNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 1]\n",
      "Creating Model...\n",
      "Selecting Parameters...\n",
      "Evaluating Model...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1557",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1557",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f1a36116f902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0muse_pretrained_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_pretrained_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0muse_glove_pretrained_embeddings_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_glove_pretrained_embeddings_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7bd15caac30a>\u001b[0m in \u001b[0;36mtrain_CNN\u001b[1;34m(data, param_grid, sent_tokenizer, use_nltk_cleaning, text_cleaning, use_tfidf_tokenizer, use_keras_tokenizer, use_pretrained_embeddings, use_glove_pretrained_embeddings_weights, use_tfidf_as_embedding_weights)\u001b[0m\n\u001b[0;32m    285\u001b[0m                                     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                                     \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                                     verbose = False)\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;31m# 5. Score Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Data_Science\\ClassificationProblems\\Sentiment_Analysis\\functions\\modelling.py\u001b[0m in \u001b[0;36mhyperparameter_optimization\u001b[1;34m(X_train, Y_train, X_test, Y_test, epochs, batch_size, param_grid, cv, n_iter, verbose)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evaluating Model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# Fit Selected Model with Random Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# Predict Y values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0;32m    269\u001b[0m                                                 random_state=rnd):\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_lists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1557"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "if make_all_other_classes_1:\n",
    "\n",
    "    remove_class_0_case_1 = False\n",
    "\n",
    "    class_weight = {0: config['params']['SVM']['class_weights_2']['0'],\n",
    "                    1: config['params']['SVM']['class_weights_2']['1']}\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_1,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            store_keras_tokenizer = store_keras_tokenizer,\n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    # if running_SVM:\n",
    "\n",
    "    #     train_SVM(data = data, \n",
    "    #         C = C, \n",
    "    #         kernel = kernel, \n",
    "    #         degree = degree, \n",
    "    #         gamma = gamma, \n",
    "    #         class_weight = class_weight,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "    #         imbalanced_classes = imbalanced_classes,\n",
    "    #         make_all_other_classes_1 = make_all_other_classes_1,\n",
    "    #         remove_class_0 = remove_class_0_case_1, \n",
    "    #         seed = seed, \n",
    "    #         store_SVM_model = store_SVM_model,\n",
    "    #         timestamp = timestamp,\n",
    "    #         output_path_models = output_path_models, \n",
    "    #         output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    if running_CNN:\n",
    "        \n",
    "        train_CNN(data = data, \n",
    "            param_grid = param_grid,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = False, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from postprocessing import cal_label_accuracy, store_to_pickle\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from postprocessing import cal_label_accuracy, store_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_cv = param_grid['num_filters_cv'][0]\n",
    "kernel_size_cv = param_grid['kernel_size_cv'][0]\n",
    "vocab_size = param_grid['vocab_size'][0]\n",
    "embedding_dim = param_grid['embedding_dim'][0]\n",
    "embedding_matrix = param_grid['embedding_matrix'][0]\n",
    "seq_input_len = param_grid['seq_input_len'][0]\n",
    "output_label = param_grid['output_label'][0]\n",
    "nodes_hidden_dense_layer = param_grid['nodes_hidden_dense_layer'][0]\n",
    "use_pretrained_embeddings = param_grid['use_pretrained_embeddings'][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.33932   ,  0.46586001,  0.75602001, ..., -0.26905999,\n",
       "        -1.21819997,  0.44033   ],\n",
       "       [-0.66845   , -0.68803   , -0.12052   , ..., -0.39627999,\n",
       "        -1.07819998,  0.20907   ],\n",
       "       [ 0.13523   , -0.24144   ,  0.58442003, ...,  0.17660999,\n",
       "        -0.58127999, -0.63757998],\n",
       "       ...,\n",
       "       [ 0.068305  , -0.85210001,  0.1919    , ...,  0.25569999,\n",
       "         1.12349999,  0.56221002],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "param_grid['embedding_matrix'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 50, 40)            158840    \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 49, 64)            5184      \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, 47, 16)            3088      \n_________________________________________________________________\nglobal_max_pooling1d_2 (Glob (None, 16)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 5)                 85        \n_________________________________________________________________\ndense_5 (Dense)              (None, 2)                 12        \n=================================================================\nTotal params: 167,209\nTrainable params: 8,369\nNon-trainable params: 158,840\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = modelling.create_model(num_filters_cv, kernel_size_cv, vocab_size, embedding_dim, embedding_matrix,\n",
    "                 seq_input_len, output_label, nodes_hidden_dense_layer, use_pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "data['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\nWARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 40).\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-84006beb32b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_test_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_test_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                     batch_size = data['batch_size'])\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_train_SVM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\User\\Anaconda3\\envs\\class\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "history = CNN_model.fit(data['X_train_CNN'], data['Y_train_SVM'],\n",
    "                    epochs = data['epochs'],\n",
    "                    verbose = False,\n",
    "                    validation_data = (data['X_test_CNN'], data['Y_test_CNN']),\n",
    "                    batch_size = data['batch_size'])\n",
    "\n",
    "loss, accuracy = model.evaluate(data['X_train_CNN'], data['Y_train_SVM'], verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(data['X_test_CNN'], data['Y_test_CNN'], verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'verbose'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7e74c4463fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'verbose'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'verbose'"
     ]
    }
   ],
   "source": [
    "data['verbose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selecting Parameters...\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn = modelling.create_model,\n",
    "                        epochs = data['epochs'], \n",
    "                        batch_size = data['batch_size'],\n",
    "                        verbose = False)\n",
    "\n",
    "print(\"Selecting Parameters...\")\n",
    "\n",
    "# Make Random Search Cross Validation\n",
    "grid = RandomizedSearchCV(estimator = model, \n",
    "                            param_distributions = param_grid,\n",
    "                            cv = data['cv'], \n",
    "                            n_iter = data['n_iter'],\n",
    "                            verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating Model...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "453",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 453",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-25048f5df733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evaluating Model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Fit Selected Model with Random Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_train_CNN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1468\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0;32m    269\u001b[0m                                                 random_state=rnd):\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_lists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 453"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Model...\")\n",
    "# Fit Selected Model with Random Parameters\n",
    "grid_result = grid.fit(data['X_train_CNN'], data['Y_train_CNN'], verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1026,    3,   17, ...,    0,    0,    0],\n",
       "       [  31, 1028, 1029, ...,    0,    0,    0],\n",
       "       [1031,   10,  124, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 724,   64,  725, ...,    0,    0,    0],\n",
       "       [ 486, 1267, 1834, ...,    0,    0,    0],\n",
       "       [  52, 1798, 1341, ...,    0,    0,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'num_filters_cv': [(64, 16)],\n",
       " 'kernel_size_cv': [(2, 3)],\n",
       " 'vocab_size': [5000],\n",
       " 'embedding_dim': [50],\n",
       " 'seq_input_len': [50],\n",
       " 'nodes_hidden_dense_layer': [5],\n",
       " 'use_pretrained_embeddings': [True],\n",
       " 'embedding_matrix': [[]],\n",
       " 'output_label': [2],\n",
       " 'corpus':                                                    text  label  label_orignal\n",
       " 0     stable way business life many corporate purcha...      0              0\n",
       " 1     dozens companies already learned supply demand...      0              0\n",
       " 2     capabilities profitable international business...      0              0\n",
       " 3       almost every kind manufacturer answer questions      0              0\n",
       " 4         companies already responded growing pressures      0              0\n",
       " ...                                                 ...    ...            ...\n",
       " 2001  twostep process buying leverage established pr...      1              2\n",
       " 2002  main products noncritical category office supp...      1              1\n",
       " 2003  key question respect nonproduction oriented pu...      1              1\n",
       " 2004  product category travel epenses eample pooling...      0              0\n",
       " 2005  framework agreement master contract preferred ...      0              0\n",
       " \n",
       " [2006 rows x 3 columns]}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "use_pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['X_train_CNN'], \n",
    "                                    Y_train = data['Y_train_CNN'], \n",
    "                                    X_test = data['X_test_CNN'], \n",
    "                                    Y_test = data['Y_test_CNN'] , \n",
    "                                    epochs = data['epochs'] , \n",
    "                                    batch_size = data['batch_size'],\n",
    "                                    param_grid = param_grid,\n",
    "                                    cv = data['cv'], \n",
    "                                    n_iter = data['n_iter'],\n",
    "                                    verbose = False)"
   ]
  },
  {
   "source": [
    "## Case 2: Model for 1, 2, 3, 4  and exclude 0. Convert 1, 2, 3 , 4 to 0, 1, 2, 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [3 0 2 1]\nmake_all_other_classes_1:  False\nremove_class_0 : True\nSVM Accuracy Score ->  71.85185185185186\nAccuracy for label 0 :  78.26  %\nAccuracy for label 1 :  60.0  %\nAccuracy for label 2 :  80.0  %\nAccuracy for label 3 :  70.27  %\nWritting results...\nRunning SVM Modeling \n  \n            Seed : 123\n\n            Test Accuracy : 71.8519\n\n            C : 1.0\n\n            kernel : linear\n\n            degree : 3\n \n            gamma : auto\n\n            class_weight : {0: 1, 1: 1, 2: 1, 3: 1}\n\n            sent_tokenizer : False \n   \n            use_nltk_cleaning: True\n \n            text_cleaning: False\n  \n            make_all_other_classes_1: False\n  \n            remove_class_0: True \n\n            use_tfidf_tokenizer: True\n \n            use_keras_tokenizer: False\n \n            use_pretrained_embeddings: False\n \n            use_glove_pretrained_embeddings_weights: False\n \n            use_tfidf_as_embedding_weights: False\n \n            imbalanced_classes: True\n \n            label accuracy: {0: 78.26, 1: 60.0, 2: 80.0, 3: 70.27}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if remove_class_0:\n",
    "\n",
    "    make_all_other_classes_1_case_2 = False\n",
    "    \n",
    "    # Remember that without the 0 , the other labels are reindexed\n",
    "    class_weight = {0: config['params']['data']['SVM']['class_weights_1_2_3_4']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights_1_2_3_4']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights_1_2_3_4']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights_1_2_3_4']['3']}\n",
    "\n",
    "\n",
    "    data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_2, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "        # Train and Calculate Accuracy for SVM\n",
    "    if running_SVM:\n",
    "\n",
    "        train_SVM(data = data, \n",
    "            C = C, \n",
    "            kernel = kernel, \n",
    "            degree = degree, \n",
    "            gamma = gamma, \n",
    "            class_weight = class_weight,\n",
    "            sent_tokenizer = sent_tokenizer, \n",
    "            use_nltk_cleaning = use_nltk_cleaning, \n",
    "            text_cleaning = text_cleaning, \n",
    "            use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "            use_keras_tokenizer = use_keras_tokenizer, \n",
    "            use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "            use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "            use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "            imbalanced_classes = imbalanced_classes,\n",
    "            make_all_other_classes_1 = make_all_other_classes_1_case_2,\n",
    "            remove_class_0 = remove_class_0, \n",
    "            seed = seed, \n",
    "            store_SVM_model = store_SVM_model,\n",
    "            timestamp = timestamp,\n",
    "            output_path_models = output_path_models, \n",
    "            output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "    # Train and Calculate Accuracy for CNN\n",
    "    # if running_CNN:\n",
    "        \n",
    "    #     train_CNN(data = data, \n",
    "    #         param_grid = param_grid,\n",
    "    #         sent_tokenizer = sent_tokenizer, \n",
    "    #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "    #         text_cleaning = text_cleaning, \n",
    "    #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "    #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "    #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "    #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "    #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n"
   ]
  },
  {
   "source": [
    "## Case 3: Model for all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The unique labels are  [0 4 1 3 2]\n",
      "make_all_other_classes_1:  False\n",
      "remove_class_0 : False\n",
      "SVM Accuracy Score ->  63.745019920318725\n",
      "Accuracy for label 0 :  58.97  %\n",
      "Accuracy for label 1 :  57.69  %\n",
      "Accuracy for label 2 :  63.64  %\n",
      "Accuracy for label 3 :  92.86  %\n",
      "Accuracy for label 4 :  84.85  %\n",
      "Writting results...\n",
      "Running SVM Modeling \n",
      "  \n",
      "            Seed : 123\n",
      "\n",
      "            Test Accuracy : 63.745\n",
      "\n",
      "            C : 1.0\n",
      "\n",
      "            kernel : linear\n",
      "\n",
      "            degree : 3\n",
      " \n",
      "            gamma : auto\n",
      "\n",
      "            class_weight : {0: 0.05, 1: 1, 2: 1, 3: 1, 4: 1}\n",
      "\n",
      "            sent_tokenizer : False \n",
      "   \n",
      "            use_nltk_cleaning: True\n",
      " \n",
      "            text_cleaning: False\n",
      "  \n",
      "            make_all_other_classes_1: False\n",
      "  \n",
      "            remove_class_0: False \n",
      "\n",
      "            use_tfidf_tokenizer: True\n",
      " \n",
      "            use_keras_tokenizer: False\n",
      " \n",
      "            use_pretrained_embeddings: False\n",
      " \n",
      "            use_glove_pretrained_embeddings_weights: False\n",
      " \n",
      "            use_tfidf_as_embedding_weights: False\n",
      " \n",
      "            imbalanced_classes: True\n",
      " \n",
      "            label accuracy: {0: 58.97, 1: 57.69, 2: 63.64, 3: 92.86, 4: 84.85}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    remove_class_0_case_3 = False\n",
    "    make_all_other_classes_1_case_3 = False\n",
    "\n",
    "    if config['params']['data']['SVM']['use_class_weights']:\n",
    "\n",
    "        class_weight = {0: config['params']['data']['SVM']['class_weights']['0'],\n",
    "                    1: config['params']['data']['SVM']['class_weights']['1'],\n",
    "                    2: config['params']['data']['SVM']['class_weights']['2'],\n",
    "                    3: config['params']['data']['SVM']['class_weights']['3'],\n",
    "                    4: config['params']['data']['SVM']['class_weights']['4']}\n",
    "\n",
    "\n",
    "\n",
    "        data, param_grid = preprocess(data_dir = config['params']['input_data']['data'], \n",
    "                            data = data, \n",
    "                            param_grid = param_grid, \n",
    "                            read_type = config['params']['input_data']['read_type'], \n",
    "                            sep = config['params']['input_data']['read_type'],\n",
    "                            remove_class_0 = remove_class_0_case_3,\n",
    "                            make_all_other_classes_1 = make_all_other_classes_1_case_3, \n",
    "                            running_CNN = running_CNN, \n",
    "                            running_SVM = running_SVM, \n",
    "                            timestamp = timestamp, \n",
    "                            output_path_vectorizer = output_path_vectorizer, \n",
    "                            store_tfidf_tokenizer = store_tfidf_tokenizer, \n",
    "                            file_path_glove = data['filepath'])\n",
    "\n",
    "            # Train and Calculate Accuracy for SVM\n",
    "        if running_SVM:\n",
    "\n",
    "            train_SVM(data = data, \n",
    "                    C = C, \n",
    "                    kernel = kernel, \n",
    "                    degree = degree, \n",
    "                    gamma = gamma, \n",
    "                    class_weight = class_weight,\n",
    "                    sent_tokenizer = sent_tokenizer, \n",
    "                    use_nltk_cleaning = use_nltk_cleaning, \n",
    "                    text_cleaning = text_cleaning, \n",
    "                    use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "                    use_keras_tokenizer = use_keras_tokenizer, \n",
    "                    use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "                    use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "                    use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights,\n",
    "                    imbalanced_classes = imbalanced_classes,\n",
    "                    make_all_other_classes_1 = make_all_other_classes_1_case_3,\n",
    "                    remove_class_0 = remove_class_0_case_3, \n",
    "                    seed = seed, \n",
    "                    store_SVM_model = store_SVM_model,\n",
    "                    timestamp = timestamp,\n",
    "                    output_path_models = output_path_models, \n",
    "                    output_path_parameters = output_path_parameters)\n",
    "\n",
    "\n",
    "        # Train and Calculate Accuracy for CNN\n",
    "        # if running_CNN:\n",
    "            \n",
    "        #     train_CNN(data = data, \n",
    "        #         param_grid = param_grid,\n",
    "        #         sent_tokenizer = sent_tokenizer, \n",
    "        #         use_nltk_cleaning = use_nltk_cleaning, \n",
    "        #         text_cleaning = text_cleaning, \n",
    "        #         use_tfidf_tokenizer = use_tfidf_tokenizer, \n",
    "        #         use_keras_tokenizer = use_keras_tokenizer, \n",
    "        #         use_pretrained_embeddings = use_pretrained_embeddings,\n",
    "        #         use_glove_pretrained_embeddings_weights = use_glove_pretrained_embeddings_weights,\n",
    "        #         use_tfidf_as_embedding_weights = use_tfidf_as_embedding_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  raw materials purchase is decisive for the com...      1\n",
       "1      framework agreement master contract preferred      0\n",
       "2  twostep process                               ...      2\n",
       "3  companys top management promptly ordered chang...      3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>raw materials purchase is decisive for the com...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>framework agreement master contract preferred</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>twostep process                               ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>companys top management promptly ordered chang...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "new_sentence  = pd.DataFrame({'text':[\"raw materials purchase is decisive for the company\", 'framework agreement master contract preferred', 'twostep process                                        buying leverage established provoking leverage portfolio matrix' , 'companys top management promptly ordered change purchasing policy build alternative domestic sources'],\n",
    "                            'label':[1,0,2,3]})\n",
    "\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-01-17_22-37-56'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Test Case 1\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_01') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 1, 1, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Test Case 2\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer_1234')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM_1234') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Test Case 3\n",
    "\n",
    "loaded_TFIDF_tokenizer = postprocessing.load_pickle(output_path = output_path_vectorizer , \n",
    "                                    timestamp = timestamp , \n",
    "                                    file_name = 'TFIDF_vectorizer')\n",
    "\n",
    "SVM_model =  postprocessing.load_pickle(output_path = output_path_models , \n",
    "                        timestamp = timestamp , \n",
    "                        file_name = 'SVM') \n",
    "\n",
    "postprocessing.classify_new_sentences(new_sentence = new_sentence , model = SVM_model, vectorizer = loaded_TFIDF_tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('class': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a859f67d4acffa6abd10af9224a6e751dd4159d06149102e129e99dcb493c1c8"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}